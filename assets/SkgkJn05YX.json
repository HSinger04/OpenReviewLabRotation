{
    "comment_0": {
        "comment": "Thanks for your reply!",
        "title": "Thanks"
    },
    "comment_1": {
        "comment": "> There is an \"overfitting\" phenomenon [3] of the adversarial examples generated by PGD and CW. So changing the network architecture (e.g., masking neurons) could be useful to defend against them. But it's not clear whether the proposed method is generally robust to more powerful transfer-based black-box attacks.\n\nFor your concern, we generate adversarial examples by the attack method proposed in [1] and use them to attack both normal and Random Masked network. The results are listed in the table below. Networks in the first row are the source models from which we generate adversarial examples by MI-FGSM ([1]). The results show that Random Mask is still effective against MI-FGSM ([1]).\n\n---------------------------------------------------------------------\n|                              | DenseNet | SENet   | TestAcc |\n| Normal ResNet |    12.58%   |   8.44%  |    95.33  | \u2028\n| Random Mask   |    58.11%   | 50.81% |    93.39   |\n---------------------------------------------------------------------\n\n> Most of the experiments are based on the transfer-based black-box setting. I think the author should not claim their method is robust against general black-box attacks since there are score-based [1] and decision-based [2] methods.\n\nPlease refer to the last paragraph of our reply to AnonReviewer1.\n\n[1] Dong et al., Boosting Adversarial Attacks with Momentum. CVPR 2018.\n",
        "title": "Still effective against the mentioned transfer-based black-box attack"
    },
    "comment_10": {
        "comment": "Thanks for your reply.\n\nThe theoretical analysis of Random Mask is highly dependent on the theory of CNN which is not clear yet. Therefore, we only show some of our intuition (See Section 2), and we used the Random Shuffle experiment (See Appendix A) to verify our intuition that our model learns more \u201creasonable\u201d features than a normal CNN. A brief restatement of our insights can be found in our reply to AnonReviewer2. Also, we regard the theoretical analysis of Random Mask as an interesting future work and further discussion is welcomed.\n\nWe have mentioned in the paper that Random Mask is an efficient method with simple implementation while enhances robustness significantly. As for your concern on the black-box attack methods, you may refer to the last paragraph of our reply to AnonReviewer1.",
        "title": "Heuristic but meaningful results"
    },
    "comment_11": {
        "comment": "Please refer to the last paragraph of our reply to AnonReviewer1.\n",
        "title": "Thanks for your comment"
    },
    "comment_12": {
        "comment": "It seems there are misunderstandings of our method. For your two major concerns, we first give a brief answer and then provide detailed explanations.\n\n1. Random Mask is NOT a weight-dropping pruning method.\n2. We conduct experiments comparing Random Mask with pruning methods (both you mentioned and those commonly used). It turns out a network with Random mask is far more robust than pruning.\n\n\nDetailed Explanations:\n\n1. Random Mask is a simple but carefully designed method. It removes some nodes in the shallow convolutional layers whose receptive fields are relatively small. This is very different from typical pruning methods which drop weights, remove channels or restrict connections between channels in two adjacent layers. The key idea of Random Mask is that by removing a part of such neurons, the remaining neurons in the shallow layers can not only response to features, but also automatically record the locations of the features. As a result, these remaining neurons in shallow layers together detect the spatial structure of the features, much better than the standard neural networks.\n\nThe motivation of our design comes from the recent observation of adversarial examples. In many cases, the adversarial examples change a patch of the original image so that the perturbed patch looks like a small part of the incorrectly classified object. This perturbed patch, although contains crucial features of the incorrectly classified object, usually appears at the wrong location and does not have the right spatial structure with other parts of the image. For example, the adversarial example of a panda image is misclassified as a monkey because a patch of the panda skin is perturbed adversarially so that it alone looks like the monkey\u2019s face. However, this patch does not form a right structure of a monkey with other parts of the images (see Figure 11 in [1]).\n\nIn sum, current deep neural networks are strong at detecting features, but relatively weak at telling if the spatial location/structure is right. Random Mask tries to strengthen the ability of neural networks in utilizing the spatial information.\n\n2.The experimental results comparing Random Mask with typical pruning methods are given below. Common pruning methods do not improve the robustness of neural networks significantly, while a network with Random Mask is far more robust.\n\nWe test the black-box defense ability of a ResNet-18 with an expander graph compressing all connections between channels by a factor of 2 (following the method proposed in [2]). For comparison, we also list the performance of a ResNet-18 which prunes whole channels in the shallow layers (i.e. Shallow_{DC} in our paper) and a ResNet-18 equipped with Random Mask. The results are listed in the table below. Networks in the first row are the source models to generate adversarial examples by PGD with perturbation scale of 16, step size of 1 and 20 steps.\n \n----------------------------------------------------------------------------\n|\t                            |  DenseNet  |   SENet    |   TestAcc |\n| Normal  ResNet    |      2.96%      |    1.38%   |   95.33% |         \n| Expander ResNet |  \t  3.13%       |    1.46%   |   94.99% |\n| Pruning Channel  |     4.68%       |    2.13%   |   94.97% |\n| Random Mask       |   26.50%      |   21.42%  |   93.39% |\n----------------------------------------------------------------------------\n\nAs for your suggestion, we are not sure what hypotheses you hoped to verify by trying dropout at test time. Nonetheless, we think trying dropping at test time is similar to Stochastic Activation Pruning ([3]). In their work, they tested SAP in terms of black-box defense (Figure 1 (c) SAP-100 vs Dense in [3]), yet the performance is not as good as our results when the perturbation scale is 8, 16 and 32. Also, directly dropping at test time and scaling up the remaining will incur a significant drop in test accuracy. We tried to mask out 50% of the neurons in the shallow blocks of a ResNet-18 at test time only, and scale up the rest by a factor of 2. It turned out that the test accuracy dropped to around 20%, which is not acceptable.\n\nFurther discussion is welcomed if our reply does not address your concerns.\n\n[1]Liu, Mengchen, et al. \"Analyzing the Noise Robustness of Deep Neural Networks.\" arXiv preprint arXiv:1810.03913(2018).\n[2]Prabhu, Ameya, Girish Varma, and Anoop Namboodiri. \"Deep Expander Networks: Efficient Deep Networks from Graph Theory.\" arXiv preprint arXiv:1711.08757 (2017).\n[3]Dhillon, Guneet S., et al. \"Stochastic activation pruning for robust adversarial defense.\" arXiv preprint arXiv:1803.01442 (2018).\n",
        "title": "Random Mask does NOT reduce the number of parameters"
    },
    "comment_13": {
        "comment": "Thanks for your comment.\n\nIn [1], SAP is applied to pretrained networks without fine tuning. This can be interpreted as randomly dropping some neurons and scaling up the rest at test time. Our approach, however, randomly masks out neurons before training and thus changes the network structure. Hence it is essentially different from dropout at test time. In terms of performance, SAP decreases test accuracy significantly if the percentage of sampled neurons is low, while our model preserves high test accuracy even if the drop ratio is 90%. Our model also has better black-box defense performance than SAP (See Figure 1 (c) SAP-100 vs Dense in [1]). You may compare the defense success rates of our model and those of SAP since the two models are both tested using perturbation scale of 8, 16 and 32 on CIFAR-10. \n\n[1] Dhillon, Guneet S., et al. \"Stochastic activation pruning for robust adversarial defense.\" arXiv preprint arXiv:1803.01442 (2018).\n",
        "title": "Random Mask is essentially different from SAP"
    },
    "comment_14": {
        "comment": "Thanks for your review. \n\n> The caption of Table 2 could be more explicit : what are the presented percent?\n\nThanks for your suggestion on the caption of Table 2. We have fixed it.\n\n> A comparison to a regular network with the same number of free parameters as the masked network could give insight on this aspect.\n\nWe already compared the performance of our structure to that of a regular network with the same number of neurons in Section 3.3. The control group is called Channel Mask, which means randomly dropping out whole channels (or kernels, equivalently) according to the same ratio. The results (0.5-Shallow and 0.5-Shallow_{DC}) show that simply reducing the number of neurons without breaking the symmetry of channels cannot significantly enhance defense performance. We hope that the comparisons made in Section 3.3 and the full information on experiments presented in Appendix F.5 can bring about insights on how to improve the robustness of a network via changing its structure. \n\n> mainly tested on a single architecture (ResNet) and on a single database CIFAR.\n\nThanks for your suggestion that Random Mask should be tested on architectures other than ResNet-18, and on datasets other than CIFAR-10. In the new version of our paper, we have added experiments on CIFAR-10 and MNIST with Random Mask applied to ResNet-50, DenseNet, SENet and VGG.\n\n\n> Maybe not robust against the latest techniques of adversarial attack.\n\nWe have tested the robustness of CNNs with Random Mask with respect to black-box defense on three popular attack methods (FGSM, PGD and CW), and most of the results are listed in Appendix F.5. In particular, [1] suggested that PGD is \u201ca \u2018universal\u2019 adversary among first-order approaches\u201d. Besides, our model is able to effectively defend against Gaussian random noise and to generate human-fooling adversarial examples. As for your suggestion to test on more advanced black-box attacks, we think they are out of the scope of this work since these methods have few baselines to compare with. Most works concerning the robustness of neural networks focus on the three attack methods mentioned above. In our paper, the black-box defense mainly serves as an approach to evaluating robustness, and we believe the three attack methods we used are sufficient for this purpose.\n\n[1] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n",
        "title": "Your suggestions have been accounted for in our revised version"
    },
    "comment_15": {
        "comment": "Thanks for your suggestion. Among the other pieces of work mentioned in [1], there is indeed one work [3] which combines two techniques, adversarial training and thermometer encoding, to achieve better defense performance than [2]. \n\n1.We conducted experiments comparing Random Mask with [3] and found that our method is more robust. Please see the table attached below.\n\n2.We would like to emphasize that Random Mask is NOT a defense method. A network with Random Mask is an architecture that is designed to be as robust as possible by itself, without using any defense method. Of course, Random Mask can be combined with existing defense methods, for example adversarial training ([2]) which we compared with in the paper, to achieve even better results.\n\n-----------------------------------------------------------------------\n|                                      |  FGSM   |   PGD  | TestAcc |\n| Random Mask           |  91.31    |   93.67 |   91.86   |\n-----------------------------------------------------------------------\n|                                      |      DefenseRate  | TestAcc |\n|Thermometer(16) [3]|           88.25          |   89.88   |\n|Thermometer(32) [3]|           86.06          |   90.30   |\n-----------------------------------------------------------------------\n\t \t \t\n[3] uses both thermometer encoding and adversarial training on a Wide ResNet with a width factor of 4. For comparison, we tested on the same network structure with Random Mask applied to the shallow layers of it. The performance data of [3] are found in Table 12 in [3]. However, what attack methods were used to obtain the defense rates shown in Table 12 is not clear. There is a contradiction between the claimed method (PGD) and the method in [2] (FGSM) which they compared to in Table 12. Therefore, we listed the defense rates of our model against both attack methods.\n\n[1]Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" arXiv preprint arXiv:1802.00420 (2018).\n[2]Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n[3]Buckman, Jacob, et al. \"Thermometer encoding: One hot way to resist adversarial examples.\" (2018).\n",
        "title": "SOTA performance"
    },
    "comment_16": {
        "comment": "Thanks for your review.\n\n> it's not clear why the method works besides some not-yet-validated hypotheses.\n\nAlthough our method seems simple, it is carefully designed. The method removes some neurons in the *shallow* layers of the neural network. We emphasize that the neurons in the shallow layers have relatively small receptive fields. Therefore, by removing a part of such neurons, the remaining neurons in the shallow layers not only response to features, but also automatically record the locations of the features. As a result, these remaining neurons in shallow layers together detect the *spatial structure* of the features, much better than the standard neural networks.\n\nThe motivation of our design comes from the recent observation ([1]) of adversarial examples. In many cases, the adversarial examples change a patch of the original image so that the perturbed patch looks like a small part of the incorrectly classified object. This perturbed patch, although contains crucial features of the incorrectly classified object, usually appears at the wrong location and does not have the right spatial structure with other parts of the image. For example, the adversarial example of a panda image is misclassified as a monkey because a patch of the panda skin is perturbed adversarially so that it alone looks like the monkey\u2019s face. However, this patch does not form a right structure of a monkey with other parts of the images (see Figure 11 in [1]).\n\nIn sum, current deep neural networks are strong at detecting features, but relatively weak at telling if the spatial location/structure is right. Random Mask tries to strengthen the ability of neural networks in utilizing the spatial information. \n\n> graybox results seem to suggest that the effectiveness of the method is due to the baseline...\n\nThe grey-box attacks are very similar to white-box attacks in our setting. We demonstrate in the paper that the adversarial examples (eps = 16,32)  generated by the white-box attacks for Random Mask often fool human as well. We then raise the following questions: 1) Should these adversarial examples be classified as their original categories? 2) How to evaluate the robustness of a method? 3) Can we entirely rely on the currently used performance measures?\n\nIn sum, one of the major goal of this paper is to move a tiny step towards a better understanding of the problem of adversarial example.\n\n[1] Liu, Mengchen, et al. \"Analyzing the Noise Robustness of Deep Neural Networks.\" arXiv preprint arXiv:1810.03913(2018).\n",
        "title": "Insights of the Random Mask"
    },
    "comment_17": {
        "comment": "The proposed approach is very similar to Stochastic Activation Pruning at ICLR'18, which was one of the defenses shown to be broken by Athalye et al. 2018. Unfortunately the authors do not run the attacks that beat SAP on their model which makes it difficult to know if it will be effective.",
        "title": "Very similar to Stochastic Activation Pruning"
    },
    "comment_18": {
        "comment": "Besides, it concerns me that the paper didn't make a comparison with SOTA defenses besides Madry et al.\n\nSince Athalye et al. (2018) only invalidates other defense methods in the white-box setting, some of them could still be robust in the black-box setting, I assume?",
        "title": "Lack of SOTA black box defenses"
    },
    "comment_19": {
        "comment": "Despite making claims about being robust to black-box attacks, this paper does not seem to actually perform any state of the art black-box attacks. See for example\n\nDecision Based Adversarial Attacks ICLR'18\nAdversarial Risk and the Dangers of Evaluating Against Weak Attacks ICML'18\nBlack-box Adversarial Attacks with Limited Queries and Information ICML'18",
        "title": "Lack of SOTA black box attacks"
    },
    "comment_2": {
        "comment": "Most of the experiments are based on the transfer-based black-box setting. I think the author should not claim their method is robust against general black-box attacks since there are score-based [1] and decision-based [2] methods.\n\nHowever, as far as I'm concerned, PGD and CW attacks are popular methods in the white-box setting. Many transfer-based black-box attacks [3,4] are proposed and studied in the literature. There is an \"overfitting\" phenomenon [3] of the adversarial examples generated by PGD and CW. So changing the network architecture (e.g., masking neurons) could be useful to defend against them. But it's not clear whether the proposed method is generally robust to more powerful transfer-based black-box attacks.\n\n[1] Ilyas et al., Black-box Adversarial Attacks with Limited Queries and Information. ICML 2018.\n[2] Brendel et al., Decision-based Adversarial Attacks. ICLR 2018.\n[3] Dong et al., Boosting Adversarial Attacks with Momentum. CVPR 2018.\n[4] Xie et al., Improving Transferability of Adversarial Examples with Input Diversity. Arxiv 2018.",
        "title": "More powerful transfer-based black-box attacks"
    },
    "comment_20": {
        "comment": "I think 8/255 is a standard because the state-of-the-defense, i.e., PGD adversarial training, achieves 44.71% under DAA when the perturbation is 8/255. (under white-box adaptive setting)\nI haven't seen a work that can really outperform PGD adversarial training until now. You can refer to the \"obfuscated gradients\" paper https://arxiv.org/abs/1802.00420 and its Github page https://github.com/anishathalye/obfuscated-gradients.\n",
        "title": "On Cifar10, 8/255 makes more sense"
    },
    "comment_21": {
        "comment": "1. Why does random mask improve the network robustness against adversarial samples? The reason is not intuitive to me, is there any precise theoretical support\uff1f\n\n2. Many defenses can work under simple black settings, so what is the advantage of your framework? Besides, there are some advanced black-box settings, e.g., 1. using certain inputs and the outputs of the model to train a similar model as the target 2. attack an ensemble of models as in https://arxiv.org/pdf/1710.06081.pdf  3. GAN-based black-box attack as in https://arxiv.org/abs/1801.02610  \nSince you claim your defense achieves state-of-the-art performance against black-box adversarial attacks, have you tested those advanced black-box settings?",
        "title": "Interesting but few questions"
    },
    "comment_22": {
        "comment": "It seems that Tab.6 is for MNIST where small perturbations are no use since the data can be easily binarized. For datasets like Cifar, 16/255 is certainly reasonable in my experience. I'm sorry if I miss something you mentioned and I'm pleasure to have further discussion. ",
        "title": "Tab.6 is for MNIST"
    },
    "comment_23": {
        "comment": "The paper said the scale is 0.3x255 (caption of Tab.6), which I think is pretty large. The images are hardly intelligible.\n\nAccording to previous works, a scale smaller than 0.063 (16/255) is reasonable.",
        "title": "same question"
    },
    "comment_24": {
        "comment": "I like that you test using an attack based on transfer from one model  that was trained with Random Mask to another model that also has Random Mask. This helps to show that the defense works even if the attacker knows you are using Random Mask, i.e. that the defense hasn't just made the model *different* from a normal CNN. ",
        "title": "I like appendix G.3"
    },
    "comment_25": {
        "comment": "Several images in this paper show that with large perturbations, humans also change their output class. Does this paper also evaluate with small perturbations, that are more likely to be class-preserving? For example, Madry et al use epsilon=8, and it would be nice to compare directly to this threat model. Sorry if this is already in the paper and I've missed it.",
        "title": "Are there any experiments with small perturbations?"
    },
    "comment_26": {
        "comment": "Which attack is used in Figure 7? For example, is it a black box attack? If so, which model are the examples transferred from? What is the size of the max norm constraint?",
        "title": "Minor clarification question"
    },
    "comment_27": {
        "comment": "Is Random Mask intended to be specific to convolutional neural networks? It seems like if you applied Random Mask to a fully connected layer, it would be equivalent to just using a smaller layer. I don't intend for this question to affect whether the paper is accepted or rejected, just asking to further my own understanding. I'm sorry if this is already explained in the paper and I've missed it.",
        "title": "Minor clarification question"
    },
    "comment_28": {
        "comment": "This sentence could be misconstrued as implying that adversarial examples are specific to deep neural networks: \"Despite the great success in numerous applications, recent studies have found that deep CNNs are vulnerable to some well-designed input samples named as Adversarial Examples\". It would be better to rewrite this to say something like \"all known machine learning models including deep CNNs\". I don't intend for this piece of feedback to influence the reviewers toward accepting or rejecting the paper, it is just a suggestion for revision to slightly improve clarity.",
        "title": "Minor suggestion for revision"
    },
    "comment_3": {
        "comment": "Thanks for updating your review.\n\n> the approach is highly sensitive to the hyperparameter \"drop rate\" and there is no way to find a good value for it.\n\nRandom Mask is not highly sensitive to the drop rate. Please refer to Figure 14 in our paper for results under the same setting but with different drop rates. The test accuracy monotonically decreases as the defense rate increases along with the drop rate. Therefore, the appropriate value for the drop rate mainly depends on the relative importance of test accuracy and robustness. For example, if a task mainly requires high defense performance instead of high test accuracy, a large drop rate should be used.  \n\n> I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.\n\nWe have described our insights on why the proposed method works both in the paper and in our rebuttals which are supported by extensive experiments. Yet we believe that at present, the primary question for adversarial example research is to understand the prevalent existence of adversarial examples with small perturbations against various machine learning methods. Only when this primary question is well-understood can one further discuss why a new method is more robust. However, there is currently no satisfactory theory or insight on why adversarial examples exist. To make the question more specific, let us take the training data of CIFAR-10 as an example. A simple experiment shows that the average \\ell_\\infty distance between two images from different categories is larger than 100. Even the smallest \\ell_\\infty distance between two differently categorized images is 54, the half of which is significantly larger than 16, the common perturbation scale required to fool the model on CIFAR-10 by common attack methods. This fact demonstrates that the classifiers we currently use have a lot of room for improvement in terms of robustness. Random Mask is an attempt to improve robustness of common classifiers while maintaining generalization.",
        "title": "Not ad hoc, Random Mask improves robustness of existing CNN architectures"
    },
    "comment_4": {
        "comment": "Here is a summary of the revision:\n1. We provide a detailed explanation of why Random Mask is robust in Section 2. \n2. We provide more results of Random Mask applied to different network structures (ResNet-50, DenseNet-121, SENet-18, VGG-19) on CIFAR-10 and MNIST datasets in Appendix F.2. These results are consistent with our original version on ResNet-18.\n",
        "title": "Revision uploaded"
    },
    "comment_5": {
        "comment": "Thanks for your comment. Actually our paper included experiments with perturbation scale 8, which should address your concern. Please refer to Section 3.1.1 and Appendix F.1, F.5..",
        "title": "Please refer to our results"
    },
    "comment_6": {
        "comment": "Since we are not trying to define \u201cadversarial examples\u201d in that sentence, we think using the expression \u201cnamed as\u201d is fine. We did not mention the existence of adversarial examples in other machine learning models because our paper mainly focus on CNNs. Further discussion is certainly welcomed if you have other suggestions.",
        "title": "Thanks for your suggestion"
    },
    "comment_7": {
        "comment": "Thanks for your comment. Please see Section 2 for the intuition of Random Mask. It is indeed based on a convolutional network structure. Nonetheless, it is definitely worth trying to apply Random Mask or some similar ideas to network structures other than CNNs, and we will surely be pleased if more works concerning Random Mask come out in the future.",
        "title": "Based on convolutional network but potential for generalizing"
    },
    "comment_8": {
        "comment": "Thanks for your reply. In our revision, Figure 7 is changed to Figure 14. It serves as a complement of Table 1 in the main body, and the attack also follows [1]\u2019s setting which is mentioned in the caption of Table 1 and is elaborated in Appendix F.1.\n\n[1]Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n",
        "title": "Also under Madry\u2019s setting"
    },
    "comment_9": {
        "comment": "Thanks for your replies and discussions. We think that there might be some misunderstanding here. The presented adversarial examples from the CIFAR-10 dataset are generated using perturbation scale \u03b5=16, 32, yet we have evaluated our model in terms of defense success rate using different \u03b5 values ranging from 4 to 32 - especially the results under the setting of [1] are presented in Section 3.1.1. We have also observed adversarial examples generated using different \u03b5 values. In fact, when \u03b5=8, most adversarial examples from CIFAR-10 against our model are hard for humans to classify. The reason why we present images after larger perturbations is that we found examples capable of \u201cfooling\u201d human eyes using \u03b5 values larger than 8 (See Figure 1). Moreover, as is mentioned in one of your replies, 16/255 is reasonable for ordinary networks. However, in contrast to our model, adversarial examples generated against *normal* CNNs with \u03b5=16 are similar to the original images added with some noise which can be easily ignored by humans.\n\nBesides, as is also mentioned in one of your replies, Table 5 (which extends Table 6 in the original version) is for MNIST, and the perturbation scale used there should not be compared with that used in experiments on CIFAR-10. \n\n[1]Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n",
        "title": "Experimental results are under several common settings"
    },
    "metareview_0": {
        "confidence": "5: The area chair is absolutely certain",
        "metareview": "This paper presents a new technique for modifying neural network structure, and suggest that this structure provides improved robustness to black-box attacks, as compared to standard architectures. The paper is very thorough in its experimentation, and the method is simple and quite easy to understand. It also raises some important questions about adversarial examples. \n\nHowever, there are serious concerns regarding the evaluation methodology. In particular, the authors claim \"black-box robustness\" but do not test against any query-based attacks, which are known to perform better against gradient masking-based adversarial defenses. Furthermore, it is not clear why one would expect adversarial examples to transfer between models representing two completely different functions (i.e. from a standard model to a random mask model). So, the gray-box evaluation is much more informative and, unfortunately, random-mask seems to provide little to no robustness in this setting.\n\nGiven how fundamental sound and convincing evaluation is for proposed defense methods, the submission is not ready for publication yet. In particular, the authors are urged to (a) evaluate on stronger black-box attacks, and (b) compare to a baseline that is known to be non-robust, (e.g. JPEG encoding or SAP), to verify that these results are actually due to black-box robustness and not simply obfuscation.",
        "recommendation": "Reject",
        "title": "Some interesting ideas but is not mature enough for publication"
    },
    "paper_0": {
        "TL;DR": "We propose a technique that modifies CNN structures to enhance robustness while keeping high test accuracy, and raise doubt on whether current definition of adversarial examples is appropriate by generating adversarial examples able to fool humans.",
        "_bibtex": "@misc{\nluo2019random,\ntitle={{RANDOM} {MASK}: Towards Robust Convolutional Neural Networks},\nauthor={Tiange Luo and Tianle Cai and Mengxiao Zhang and Siyu Chen and Liwei Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=SkgkJn05YX},\n}",
        "abstract": "Robustness of neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed  perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. In this paper, we design a new CNN architecture that by itself has good robustness. We introduce a simple but powerful technique, Random Mask, to modify existing CNN structures. We show that CNN with Random Mask achieves state-of-the-art performance against black-box adversarial attacks without applying any adversarial training. We next investigate the adversarial examples which \u201cfool\u201d a CNN with Random Mask. Surprisingly, we find that these adversarial examples often \u201cfool\u201d humans as well. This raises fundamental questions on how to define adversarial examples and robustness properly.",
        "authorids": [
            "luotg@pku.edu.cn",
            "caitianle1998@pku.edu.cn",
            "zhan147@usc.edu",
            "siyuchen@pku.edu.cn",
            "wanglw@cis.pku.edu.cn"
        ],
        "authors": [
            "Tiange Luo",
            "Tianle Cai",
            "Mengxiao Zhang",
            "Siyu Chen",
            "Liwei Wang"
        ],
        "keywords": [
            "adversarial examples",
            "robust machine learning",
            "cnn structure",
            "metric",
            "deep feature representations"
        ],
        "paperhash": "luo|random_mask_towards_robust_convolutional_neural_networks",
        "pdf": "/pdf/065faae5add172add31a55eb6b5c3e2b514086d1.pdf",
        "title": "RANDOM MASK: Towards Robust Convolutional Neural Networks"
    },
    "pdf": "RANDOM MASK: TOWARDS ROBUST CONVOLUTIONAL\nNEURAL NETWORKS\n\nTiange Luo1\u2217, Tianle Cai1\u2217, Mengxiao Zhang2, Siyu Chen1, Liwei Wang1\n1Peking University, 2University of Southern California\n1{luotg,caitianle1998,siyuchen,wanglw}@pku.edu.cn, 2zhan147@usc.edu\n\nABSTRACT\n\nRobustness of neural networks has recently been highlighted by the adversarial examples,\ni.e., inputs added with well-designed perturbations which are imperceptible to humans\nbut can cause the network to give incorrect outputs.\nIn this paper, we design a new\nCNN architecture that by itself has good robustness. We introduce a simple but powerful\ntechnique, Random Mask, to modify existing CNN structures. We show that CNNs with\nRandom Mask achieve state-of-the-art performance against black-box adversarial attacks\nwithout applying any adversarial training. We next investigate the adversarial examples\nwhich \u201cfool\u201d a CNN with Random Mask. Surprisingly, we \ufb01nd that these adversarial\nexamples often \u201cfool\u201d humans as well. This raises fundamental questions on how to de\ufb01ne\nadversarial examples and robustness properly.\n\n1\n\nINTRODUCTION\n\nDeep learning (LeCun et al., 2015), especially deep Convolutional Neural Network (CNN) (LeCun et al.,\n1998), has led to state-of-the-art results spanning many machine learning \ufb01elds, such as image classi-\n\ufb01cation (He et al., 2016; Hu et al., 2017b; Huang et al., 2017; Simonyan & Zisserman, 2014), object\ndetection (Redmon et al., 2016; Girshick, 2015; Ren et al., 2015), image captioning (Vinyals et al., 2015; Xu\net al., 2015) and speech recognition (Bengio et al., 2003; Hinton et al., 2012).\n\nDespite the great success in numerous applications, recent studies have found that deep CNNs are vulnerable\nto some well-designed input samples named as Adversarial Examples (Szegedy et al., 2013) (Biggio et al.,\n2013). Take the task of image classi\ufb01cation as an example, for almost every commonly used well-performed\nCNN, attackers are able to construct a small perturbation on an input image to cause the model to give an\nincorrect output label. Meanwhile, the perturbation is almost imperceptible to humans. Furthermore, these\nadversarial examples can easily transfer among different kinds of CNN architectures (Papernot et al., 2016b).\n\nSuch adversarial examples raise serious concerns on deep neural network models as robustness is crucial in\nmany applications. Just as Goodfellow (2018) suggests, both robustness and traditional supervised learning\nseem fully aligned. Recently, there is a rapidly growing body of work on this topic. One important line of\nresearch is adversarial training (Szegedy et al., 2013; Madry et al., 2017; Goodfellow et al., 2015; Huang\net al., 2015). Although adversarial training gains some success, a major dif\ufb01culty is that it tends to over\ufb01t to\nthe method of adversarial example generation used at training time (Buckman et al., 2018). Xie et al. (2017)\nand Guo et al. (2017) propose defense methods by introducing randomness and applying transformations\nto the inputs respectively. Dhillon et al. (2018) introduces random drop during the evaluation of a neural\nnetwork. However, Athalye et al. (2018) contends that such transformation and randomness only provide\na kind of \u201cobfuscated gradient\u201d and can be attacked by taking expectation over transformation (EOT) to\n\n\u2217Equal contribution.\n\n1\n\n\fget a meaningful gradient. Papernot et al. (2016a) and Katz et al. (2017) consider the non-linear functions\nin the networks and try to achieve robustness by adjusting them. There are also detection-based defense\nmethods (Metzen et al., 2017; Grosse et al., 2017; Meng & Chen, 2017), which add a process of detecting\nwhether an input is adversarial.\n\nIn this paper, different from most of the existing methods, we take another approach to tackle the adversarial\nexample problem. In particular, we aim to design a new CNN architecture which by itself enjoys robustness,\nwithout appealing to techniques such as adversarial training. To this end, we introduce Random Mask as\na new ingredient of CNN. To be speci\ufb01c, we randomly select a set of neurons and remove them from the\nnetwork before training. Then the architecture of the network is \ufb01xed during the training and testing process.\nNote that Random Mask is different from dropout which randomly masks out neurons in each step during\ntraining. In addition, Random Mask can be applied very easily to common CNN structures such as ResNet\nwith only a few changes of code. We \ufb01nd that applying Random Mask to the shallow layers of the network\nis crucial for robustness. CNNs with properly designed Random Mask are far more robust than standard\nCNNs. In fact, our experimental results demonstrate that CNNs with Random Mask achieve state-of-the-art\nresults against black-box attacks even when comparing with defense methods using adversarial training.\nFurthermore, CNNs with Random Mask maintain a high accuracy on normal test data, while low test accuracy\nis often regarded as a major weakness in many methods designed for achieving robustness.\n\nWe next take a closer look at the adversarial examples generated particularly against CNNs with Random Mask.\nWe investigate the adversarial examples that can \u201cfool\u201d our proposed architecture, i.e., the examples that are\nperturbed version of the original image, but are classi\ufb01ed to a different label by the network. Surprisingly,\nwe \ufb01nd that the adversarial examples which can \u201cfool\u201d a CNN with Random Mask often \u201cfool\u201d humans as\nwell. It is dif\ufb01cult for humans to correctly classify these adversarial example, and in many cases humans\nmake the same \u201cincorrect\u201d prediction as our network. Figure 1 shows a few adversarial examples generated\nby PGD (Basic iterative method) (Kurakin et al., 2016) with respect to a CNN with Random Mask as well as\nthe labels the network outputs. (Please also see Figure 12 in Appendix E.1 for the original images and labels\nfrom CIFAR-10.) They are different from the typical adversarial examples generated against commonly used\nCNNs, which usually look like noisy versions of the original images and are easy to be correctly classi\ufb01ed by\nhumans.\n\nThese observations raise important questions: 1) How should we de\ufb01ne adversarial examples? 2) How should\nwe de\ufb01ne robustness? Currently, an adversarial example is usually de\ufb01ned as a perturbed datum which lies\nin the neighborhood of the original data but has a different classi\ufb01cation output by the network; and the\nrobustness of a method is measured according to the proportion of these adversarial examples. However, if\nan adversarial example can also fool humans, it is more appropriate to say that the example does change\nthe semantics of the datum to a certain extent. After all, why two images close to each other in terms of\nsome distance (e.g., (cid:96)\u221e) must belong to the same class? How close should they be so that they belong to the\nsame class? Without complete answers to these questions, one should be very careful when measuring the\nrobustness of a model merely according to currently-de\ufb01ned adversarial examples. Robustness is a subtle\nissue. We argue that one needs to rethink the robustness and adversarial examples from the de\ufb01nitions.\n\nDog\n\nBird\n\nFrog\n\nDog\n\nAuto-\n\nShip\n\nDog\n\nShip\n\nBird\n\nFrog\n\nBird\n\nFigure 1: Adversarial examples (generated by PGD against a network with Random Mask) that can \u201cfool\u201d a\nCNN with Random Mask. The labels here are the outputs of the network being \u201cfooled\u201d. The original images\nfrom CIFAR-10 and more examples can be found in Figure 12 in Appendix E.1.\n\nmobile\n\n2\n\n\fOur main contributions are summarized as follows:\n\n\u2022 We develop a very simple but effective method, Random Mask. We show that combining with\nRandom Mask, existing CNNs can be signi\ufb01cantly more robust while maintaining high generalization\nperformance. In fact, CNNs equipped with Random Mask achieve state-of-the-art performance\nagainst several black-box attacks, even when comparing with methods using adversarial training\n(See Table 1).\n\n\u2022 We investigate the adversarial examples generated against CNNs with Random Mask. We \ufb01nd\nthat adversarial examples that can \u201cfool\u201d a CNN with Random Mask often fool humans as well.\nThis observation requires us to rethink what are the right de\ufb01nitions of adversarial examples and\nrobustness.\n\n2 RANDOM MASK\n\nWe propose Random Mask, a method to modify existing CNN structures. It randomly selects a set of neurons\nand removes them from the network before training. Then the architecture of the network is \ufb01xed during the\ntraining and testing process. To apply Random Mask on a selected layer Layer(j), suppose the input is Xj and\nthe output is convj(Xj) \u2208 Rmj \u00d7nj \u00d7cj . We randomly generate a binary mask mask(j) \u2208 {0, 1}mj \u00d7nj \u00d7cj\nby sampling uniformly within each channel. The drop rate of the sampling process is called the ratio (or\ndrop ratio) of Random Mask. Then we mask the neurons in position (x, y, c) of the output of Layer(j) if\nthe (x, y, c) element of mask(j) is zero. More speci\ufb01cally, after Random Mask, we will not compute these\nmasked neurons and make the next layer regard these neurons as having value zero during computation.\nA simple visualization of Random Mask is shown in Figure 2. The Random Mask in fact decreases the\ncomputational cost in each epoch since there are fewer effective connections. Note that the number of\nparameters in the convolutional kernels remains unchanged, since we only mask neurons in the feature maps.\n\nFigure 2: An illustration of Random Mask applied to three channels of a layer (neuron-wise). Note that the\nnumber of parameters in the network is not reduced after applying Random Mask.\n\nIn the standard setting, convolutional \ufb01lter will be applied uniformly to every position of the feature map of\nthe former layer. The success of this implementation is due to the reasonable assumption that if one feature\nis useful to be computed at some spatial position (x, y), then it should also be useful to be computed at\na different position (x(cid:48), y(cid:48)). Thus the original structure is powerful for feature extraction. Moreover, this\nstructure leads to parameter sharing which makes the training process more ef\ufb01cient. However, the uniform\napplication of \ufb01lter also prevents the CNN from noticing the distribution of features. In other words, the\nnetwork focuses on the existence of a kind of feature but pays little attention to how this kind of feature\ndistributes on the whole feature map (of the former layer). Yet the pattern of feature distribution is important\nfor humans to recognize and classify a photo, since empirically people would rely on some structured feature\nto perform classi\ufb01cation.\n\nWith Random Mask, each \ufb01lter may only extract features from partial positions. More speci\ufb01cally, for one\n\ufb01lter, only features which distribute consistently with the mask pattern can be extracted. Hence \ufb01lters in a\nnetwork with Random Mask may capture more information on the spatial structures of local features. Just\nthink of a toy example: imagine Random Mask for a \ufb01lter masks all the neurons but one row in the channel,\n\n3\n\n\fif a kind of feature usually distributes in a column, it can not have strong response because the \ufb01lter can only\ncapture a small portion of the feature.\n\nWe do a straightforward experiment to verify our intuition. We sample some images from ImageNet which\ncan be correctly classi\ufb01ed with high probability by both CNNs with and without Random Mask. We then\nrandomly shuf\ufb02e the images by patches, and compare the accuracy of classifying the shuf\ufb02ed images (See\nAppendix A). We \ufb01nd out that the accuracy of the CNN with Random Mask is consistently lower than that of\nnormal CNN. This result shows that CNNs without Random Mask cares more about whether a feature exists\nwhile CNNs with Random Mask will detect spatial structures and limit poorly-organized features from being\nextracted.\n\nWe further explore how Random Mask plays its role in defending against adversarial examples. Recent\nobservation (Liu et al., 2018) of adversarial examples found that these examples usually change a patch of the\noriginal image so that the perturbed patch looks like a small part of the incorrectly classi\ufb01ed object. This\nperturbed patch, although contains crucial features of the incorrectly classi\ufb01ed object, usually appears at the\nwrong location and does not have the right spatial structure with other parts of the image. For example (See\nFigure 11 in Liu et al. (2018)), the adversarial example of a panda image is misclassi\ufb01ed as a monkey because\na patch of the panda skin is perturbed adversarially so that it alone looks like the monkey\u2019s face. However,\nthis patch does not form a right structure of a monkey with other parts of the images. By the properties of\ndetecting spatial structures and limiting feature extraction, Random Mask can naturally help CNNs resist\nsuch adversarial perturbations.\n\nIn complement to the observation we mentioned above, we also \ufb01nd that most adversarial perturbations\ngenerated against normal CNNs look like random noises which do not change the semantic information of\nthe original image. In contrast, adversarial examples generated against CNNs with Random Mask tend to\ncontain some well-organized features which sometimes change the classi\ufb01cation results semantically (See\nFigure 3 and Figure 1). This phenomenon also supports our intuition that Random Mask helps to detect\nspatial structures and extract well-organized features via imposing limitations.\n\nOriginal Image\n\nGaussian Noise\n\nNormal CNN\n\nRandom Mask\n\nFigure 3: The \ufb01rst image is the original image, and the other three contain different types of small perturbations.\nBoth the two adversarial examples on the right are predicted as frog by the corresponding models. However,\nonly the image generated by the randomly masked CNN is capable of fooling humans.\n\nWhile the features that can be learned by each masked \ufb01lter is limited, the randomness helps us get plenty\nof diversi\ufb01ed patterns. Our experiments show that these limited \ufb01lters are enough for learning features. In\nother words, CNNs will maintain a high test accuracy after being applied with Random Mask. Besides,\nadding convolutional \ufb01lters may help our CNN with Random Mask to increase test accuracy (See Section 3.3).\nFurthermore, our structure is naturally compatible to ensemble methods, and randomness makes ensemble\nmore powerful (See Section 3.3).\n\nHowever, it might not be appropriate to apply Random Mask to deep layers. The distribution of features is\nmeaningful only when the location in feature map is highly related to the location in the original input image,\n\n4\n\n\fand the receptive \ufb01eld of each neuron in deep layers is too large. In Section 3.3, there are empirical results\nwhich support our intuition.\n\n3 EXPERIMENTS\n\nIn this section, we provide extensive experimental analyses on the performance and properties of Random\nMask network structure. We \ufb01rst test the robustness of Random Mask (See Section 3.1). Then we take a\ncloser look at the adversarial examples that can \u201cfool\u201d our proposed architecture (See Section 3.2). After that\nwe explore properties of Random Mask, including where and how to apply Random Mask, by a series of\ncomparative experiments (See Section 3.3). Some settings used in our experiments are listed below:\n\nNetwork Structure. We apply Random Mask to several target networks, including ResNet-18 (He et al.,\n2016), ResNet-50, DenseNet-121 (Huang et al., 2017), SENet-18 (Hu et al., 2017b) and VGG-19 (Simonyan\n& Zisserman, 2014). The effects of Random Mask on those network structures are quite consistent. For\nbrevity, we only show the defense performance on ResNet-18 in the main body and leave more experimental\nresults in the Appendix F. The 5-block structure of ResNet-18 is shown in the Appendix C.1. The blocks\nare labeled 0, 1, 2, 3, 4 and the 0th block is the \ufb01rst convolution layer. We divide these \ufb01ve blocks into two\nparts - the relative shallow ones (the 0th, 1st, 2nd blocks) and the deep ones (the 3rd, 4th blocks). For simplicity,\nwe would like to regard each of these two parts as a whole in this section to avoid being trapped by details.\nWe use \u201c\u03c3-Shallow\u201d and \u201c\u03c3-Deep\u201d to denote that we apply Random Mask with drop ratio \u03c3 to the shallow\nblocks and to the deep blocks in ResNet-18 respectively.\n\nAttack Framework. The accuracy under black-box attack serves as a common criterion of robustness. We\nwill use it when selecting model parameters and comparing Random Mask to other similar structures. To be\nmore speci\ufb01c, by using FGSM (Goodfellow et al., 2015), PGD (Kurakin et al., 2016) with (cid:96)\u221e norm and CW\nattack (Carlini & Wagner, 2016) with (cid:96)2 norm (See Appendix B for details on these attack approaches), we\ngenerate adversarial examples against different neural networks. The performances on adversarial examples\ngenerated against different networks are quite consistent. For brevity, we only show the defense performance\nagainst part of the adversarial examples generated by using DenseNet-121 on dataset CIFAR-10 in this\nsection, and leave more experimental results obtained by using other adversarial examples in the Appendix F.\nWe use FGSM16, PGD16, PGD32, CW40 to denote attack method FGSM with step size (cid:15) = 16, PGD with\nperturbation scale \u03b1 = 16 and step number 20, PGD with perturbation scale \u03b1 = 32 and step number 40, CW\nattack with con\ufb01dence \u03ba = 40 respectively. The step size of both PGD methods are selected to be (cid:15) = 1. We\nwould like to point out that these attacks are really powerful that a normal network cannot resist these attacks.\n\n3.1 ROBUSTNESS VIA RANDOM MASK\n\nRandom Mask is not specially designed for adversarial defense, but as Random Mask introduces information\nthat is essential for classifying correctly, it also brings robustness. As mentioned in Section 2, normal CNN\nstructures may allow adversary to inject features imperceptible to humans into images that can be recognized\nby CNN. Yet Random Mask limits the process of feature extraction, so noisy features are less likely to be\npreserved.\n\n3.1.1 ROBUSTNESS TO BLACK-BOX ATTACK\n\nThe results of our experiments show the strengths of applying Random Mask to adversarial defense. In fact,\nRandom Mask can help existing CNNs reach state-of-the-art performance against the black-box attacks we\nuse (See Table 1). In Section 3.3, we will provide more experimental results to show that this asymmetric\nstructure performs better than normal convolution and enhances robustness.\n\n5\n\n\fModel\n\nFGSM\n\nPGD\n\nTest Accuracy\n\nNormal ResNet-18\nVanilla (Madry)\nRandom Mask\n\n26.99% 7.56%\n85.60% 86.00%\n86.31% 90.30%\n\n95.33%\n87.30%\n90.08%\n\nTable 1: Performance of black-box defense under the setting of Madry et al. (2017) (See Appendix F.1 for the\ncomplete setting). We use model under adversarial training in Madry et al. (2017) as a vanilla model. It is\nregarded as a state-of-the-art adversarial defense method. Our model is only trained on clean data. The ratio\nof Random Mask here is selected to balance the performance of robustness and generalization. See Figure 14\nin Appendix F.1 for results on the performance of Random Mask with different ratios.\n\n3.1.2 ROBUSTNESS TO RANDOM NOISE\n\nBeside the robustness against black-box attack, we also evaluate the robustness to random noise. Note that\nalthough traditional network structures are vulnerable to adversarial examples, they are still robust to the\nimages perturbed with small Gaussian noises. To see whether our structure also enjoys such property, or\neven has better robustness in this sense, we feed input images with random Gaussian noises to networks with\nRandom Mask. More speci\ufb01cally, in order to obtain noises of scales similar to the adversarial perturbations,\nwe generate i.i.d. Gaussian random variables x \u223c N (0, \u03c32), where \u03c3 \u2208 {1, 2, 4, 8, 12, 16, 20, 24, 28, 32},\nclip them to the range [\u22122\u03c3, 2\u03c3] and then add them to every pixel of the input image. The results of the\nexperiments are shown in Figure 4. We can see that networks with Random Mask always have higher accuracy\nthan a normal network.\n\nFigure 4: Input images with random Gaussian noises, Random Mask versus normal network. The network\nwith Random Mask has far better robustness than the original network.\n\n3.2 ADVERSARIAL EXAMPLES?\n\nWe evaluate the performance of CNNs with Random Mask under white-box attack (See Appendix F.3). With\nneither obfuscated gradient nor gradient masking, Random Mask can still improve defense performance under\nvarious kinds of white-box attack. Also, by checking adversarial images that are misclassi\ufb01ed by our network,\nwe \ufb01nd most of them have vague edges and can hardly be recognized by humans. This result coincides with\nthe theoretical analysis in Shafahi et al. (2018); Fawzi et al. (2018) that real adversarial examples may be\n\n6\n\n\finevitable in some way. See Appendix E.2 for a randomly selected set of them. In contrast, adversarial\nexamples generated against normal CNNs are more like simply adding some non-sense noise which can\nbe ignored by human. This phenomenon also demonstrates that Random Mask really helps networks to\ncatch more information related to real human perception. Moreover, just as Figure 1 shows, with the help\nof Random Mask, we are able to \ufb01nd small perturbations that can actually change the semantic meaning of\nimages for humans. So should we still call them \u201cadversarial examples\u201d? How can we get more reasonable\nde\ufb01nitions of adversarial examples and robustness? These questions seem severe due to our \ufb01ndings.\n\n3.3 PROPERTIES OF RANDOM MASK\n\nWe then show some properties of Random Mask including the appropriate positions to apply Random Mask,\nthe bene\ufb01t of breaking symmetry, the diversity introduced by randomness and the extensibility of Random\nMask via structure adjustment and ensemble methods. We conduct a series of comparative experiments and\nwe will continue to use black-box defense performance as a criterion of robustness. For brevity, we only\npresent the results of a subset of our experiments in Table 2. Full information on all the experiments can be\nfound in Appendix F.5.\n\nNetwork Structure\n\nNormal ResNet-18\n\n0.3-Shallow\n0.5-Shallow\n0.7-Shallow\n\n0.3-Deep\n0.5-Deep\n0.7-Deep\n0.3-Shallow, 0.3-Deep\n0.3-Shallow, 0.7-Deep\n0.7-Shallow, 0.7-Deep\n0.7-Shallow, 0.3-Deep\n0.5-Shallow\n0.9-Shallow\n0.5-ShallowDC\n0.9-ShallowDC\n0.5-ShallowSM\n0.9-ShallowSM\n0.5-Shallow\u00d72\n0.9-Shallow\u00d72\n0.9-Shallow\u00d74\nNormal ResNet-18EN\n0.5-Shallow\u00d72,EN\n0.5-ShallowEN\n0.9-ShallowEN\n\nCW40 Test Accuracy\nPGD32\nPGD16\nFGSM16\n2.26%\n8.23%\n14.91%\n2.96%\n23.29% 14.53%\n5.73% 36.95%\n30.86% 26.50% 10.33% 54.02%\n48.57% 47.76% 21.39% 73.70%\n1.95%\n7.88%\n14.62%\n1.30%\n4.52%\n2.57%\n10.76%\n7.19%\n2.64% 10.10%\n11.23%\n3.24%\n6.75% 29.65%\n24.15% 12.67%\n11.26%\n5.77% 23.31%\n7.94%\n27.43% 32.72% 16.26% 62.47%\n40.58% 42.95% 19.00% 68.58%\n30.86% 26.50% 10.33% 54.02%\n79.93% 83.08% 55.02% 89.67%\n12.15%\n4.05% 12.72%\n19.00% 19.33% 10.08% 44.80%\n48.86% 44.04% 19.81% 72.07%\n39.40% 50.40% 29.23% 65.38%\n20.78% 12.51%\n5.38% 34.00%\n68.83% 66.86% 37.51% 82.74%\n59.64% 59.15% 32.29% 78.88%\n1.46%\n16.24%\n8.58%\n2.22%\n5.30% 37.37%\n19.84% 11.86%\n31.38% 27.58%\n9.97% 58.07%\n81.95% 85.14% 56.02% 91.36%\n\n95.33%\n94.03%\n93.39%\n91.83%\n95.16%\n94.94%\n94.61%\n94.16%\n93.44%\n89.78%\n91.23%\n93.39%\n87.68%\n94.97%\n93.27%\n92.57%\n74.28%\n94.12%\n90.49%\n90.57%\n96.12%\n95.24%\n94.56%\n89.45%\n\n4.68%\n\nTable 2: A subset of our experiments presented in Appendix F.5 to show properties of Random Mask.\n\u03c3-ShallowDC, \u03c3-ShallowSM, \u03c3-Shallow\u00d7n and \u03c3-ShallowEN mean dropping channels with ratio \u03c3, applying\nsame mask with ratio \u03c3, increasing channel number to n times with mask ratio \u03c3 for every channel and\nensemble \ufb01ve models with different masks of same ratio \u03c3 respectively. The entries in the middle four\ncolumns are success rates of defense under different settings. This is also .\n\n7\n\n\fMasking Shallow Layers versus Masking Deep Layers. In the last paragraph of Section 2 , we give an\nintuition that deep layers in a network should not be masked. To verify this, we do extensive experiments on\nResNet-18 with Random Mask applied to different parts. We apply Random Mask with different ratios on\nthe shallow blocks and on the deep blocks respectively. Results in Table 2 accord closely with our intuition.\nComparing the success rate of black-box attacks on the model with the same drop ratio but different parts being\nmasked, we \ufb01nd that applying Random Mask to shallow layers enjoys signi\ufb01cantly lower adversarial attack\nsuccess rates. This veri\ufb01es that shallow layers play a more important role in limiting feature extraction than\nthe deep layers. Moreover, only applying Random Mask on shallow blocks can achieve better performance\nthan applying Random Mask on both shallow and deep blocks, which also veri\ufb01es our intuition that dropping\nelements with large receptive \ufb01elds is not bene\ufb01cial for the network. In addition, we would like to point out\nthat ResNet-18 with Random Mask signi\ufb01cantly outperforms the normal network in terms of robustness.\n\nRandom Mask versus Channel Mask. As our Random Mask applies independent random masks to different\nchannels in a layer, we actually break the symmetry of the original CNN structure. To see whether this\nasymmetric structure would help, we try to directly drop whole channels instead of neurons using the same\ndrop ratio as the Random Mask and train it to see the performance. This channel mask does not hurt the\nsymmetry while also leading to the same decrease in convolutional operations. Table 2 shows that although\nour Random Mask network suffers a small drop in test accuracy due to the high drop ratio, we have a great\ngain in the robustness, compared with the channel-masking network.\n\nRandom Mask versus Same Mask. The randomness in generating masks in different channels and layers\nallows each convolutional \ufb01lter to focus on different patterns of feature distribution. We show the essentialness\nof generating various masks per layer via experiments that compare Random Mask to a method that only\nrandomly generates one mask per layer and uses it in every channel. Table 2 shows that applying the same\nmask to each channel will decrease the test accuracy. This may result from the limitation of expressivity\ndue to the monotone masks at every masked layer. In fact, we can illustrate such limitation using simple\ncalculations. Since the \ufb01lters in our base network ResNet-18 is of size 3 \u00d7 3, each element of the feature\nmaps after the \ufb01rst convolutional layer can extract features from at most 9 pixels in the original image. This\nmeans that if we use the same mask and the drop ratio is 90%, only at most 9 \u00d7 10% of the input image can\nbe caught by the convolutional layer, which would cause severe loss of input information.\n\nIncrease the Number of Channels. In order to compensate the loss of masking many neurons in each\nchannel, it is reasonable that we may need more convolutional \ufb01lters for feature extraction. Therefore, we try\nto increase the number of channels at masked layers. Table 2 shows that despite ResNet-18 is a well-designed\nnetwork structure, increasing channels does help the network with Random Mask to get higher test accuracy\nwhile maintaining good robustness performance.\n\nEnsemble Methods. Thanks to the diversity of Random Mask, we may directly use several networks with the\nsame structure but different Random Masks and ensemble them. Table 2 shows that such ensemble methods\ncan improve a network with Random Mask in both test accuracy and robustness.\n\n4 CONCLUSION AND FUTURE DIRECTIONS\n\nIn conclusion, we introduce and experiment on Random Mask, a modi\ufb01cation of existing CNNs that makes\nCNNs capture more information including the pattern of feature distribution. We show that CNNs with\nRandom Mask can achieve much better robustness while maintaining high test accuracy. More speci\ufb01cally,\nby using Random Mask, we reach state-of-the-art performance in several black-box defense settings. Another\ninsight resulting from our experiments is that the adversarial examples generated against CNNs with Random\nMask actually change the semantic information of images and can even \u201cfool\u201d humans. We hope that this\n\ufb01nding can inspire more people to rethink adversarial examples and the robustness of neural networks.\n\n8\n\n\fREFERENCES\n\nAnish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security:\nCircumventing defenses to adversarial examples. CoRR, abs/1802.00420, 2018. URL http://arxiv.\norg/abs/1802.00420.\n\nYoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language\n\nmodel. Journal of machine learning research, 3(Feb):1137\u20131155, 2003.\n\nBattista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim \u02c7Srndi\u00b4c, Pavel Laskov, Giorgio Giacinto,\nand Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference on\nmachine learning and knowledge discovery in databases, pp. 387\u2013402. Springer, 2013.\n\nJacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to\n\nresist adversarial examples. 2018.\n\nNicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. CoRR,\n\nabs/1608.04644, 2016. URL http://arxiv.org/abs/1608.04644.\n\nGuneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossai\ufb01, Aran\nKhanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. arXiv\npreprint arXiv:1803.01442, 2018.\n\nAlhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classi\ufb01er. arXiv preprint\n\nRoss Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pp.\n\narXiv:1802.08686, 2018.\n\n1440\u20131448, 2015.\n\nIan Goodfellow. Defense against the dark arts: An overview of adversarial example security research and\n\nfuture research directions. arXiv preprint arXiv:1806.04169, 2018.\n\nIan Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In\nInternational Conference on Learning Representations, 2015. URL http://arxiv.org/abs/1412.\n6572.\n\nKathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On the\n\n(statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.\n\nChuan Guo, Mayank Rana, Moustapha Ciss\u00b4e, and Laurens van der Maaten. Countering adversarial images\nusing input transformations. CoRR, abs/1711.00117, 2017. URL http://arxiv.org/abs/1711.\n00117.\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\n\nGeoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew\nSenior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. IEEE Signal processing\nmagazine, 29(6):82\u201397, 2012.\n\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. CoRR, abs/1709.01507, 2017a. URL\n\nhttp://arxiv.org/abs/1709.01507.\n\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 7,\n\n2017b.\n\n9\n\n\fGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional\n\nnetworks. In CVPR, volume 1, pp. 3, 2017.\n\nRuitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesv\u00b4ari. Learning with a strong adversary. arXiv\n\npreprint arXiv:1511.03034, 2015.\n\nGuy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An ef\ufb01cient smt\nsolver for verifying deep neural networks. In International Conference on Computer Aided Veri\ufb01cation, pp.\n97\u2013117. Springer, 2017.\n\nAlexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv preprint\n\narXiv:1611.01236, 2016.\n\nYann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\n\nrecognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436, 2015.\n\nMengchen Liu, Shixia Liu, Hang Su, Kelei Cao, and Jun Zhu. Analyzing the noise robustness of deep neural\n\nnetworks. arXiv preprint arXiv:1810.03913, 2018.\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards\n\ndeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nDongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In Proceedings\nof the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135\u2013147. ACM,\n2017.\n\nJan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial\n\nperturbations. arXiv preprint arXiv:1702.04267, 2017.\n\nNicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to\nadversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy\n(SP), pp. 582\u2013597. IEEE, 2016a.\n\nNicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning: from\nphenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016b. URL http:\n//arxiv.org/abs/1605.07277.\n\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Uni\ufb01ed, real-time\nobject detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n779\u2013788, 2016.\n\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In Advances in neural information processing systems, pp. 91\u201399, 2015.\n\nAli Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial examples\n\ninevitable? arXiv preprint arXiv:1809.02104, 2018.\n\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\n\narXiv preprint arXiv:1409.1556, 2014.\n\nChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,\nand Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013. URL http:\n//arxiv.org/abs/1312.6199.\n\n10\n\n\fOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image\ncaption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n3156\u20133164, 2015.\n\nCihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan L. Yuille. Mitigating adversarial ef-\nfects through randomization. CoRR, abs/1711.01991, 2017. URL http://arxiv.org/abs/1711.\n01991.\n\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In\nInternational conference on machine learning, pp. 2048\u20132057, 2015.\n\n11\n\n\fA RANDOM SHUFFLE\n\nFigure 5: An example image that is randomly shuf\ufb02ed after being divided into 1 \u00d7 1, 2 \u00d7 2, 4 \u00d7 4 and 8 \u00d7 8\npatches respectively.\n\nIn this part, we show results of our Random Shuf\ufb02e experiment. Intuitively, by dropping randomly selected\nneurons in the neural network, we may let the network learn the relative margins and features better than\nnormal networks. In randomly shuf\ufb02ed images, however, some global patterns of feature distributions\nare destroyed, so we expect that CNNs with Random Mask would have some trouble extracting feature\ninformation and might have worse performance than normal networks. In order to verify our intuition, we\ncompare the test accuracy of a CNN with Random Mask to that of a normal CNN on randomly shuf\ufb02ed\nimages. Speci\ufb01cally speaking, in the experiments, we \ufb01rst train a 0.7-Shallow network along with a normal\nnetwork on ImageNet dataset. Then we select 5000 images from the validation set which are predicted\ncorrectly with more than 99% con\ufb01dence by both normal and masked networks. We resize these images to\n256 \u00d7 256 and then center crop them to 224 \u00d7 224. After that, we random shuf\ufb02e them by dividing them\ninto k \u00d7 k small patches k \u2208 {2, 4, 8}, and randomly rearranging the order of patches. Figure 5 shows one\nexample of our test images after random shuf\ufb02ing. Finally, we feed these shuf\ufb02ed images to the networks and\nsee their classi\ufb01cation accuracy. The results are shown in Table 3.\n\nModel\n\n2 \u00d7 2\n\n4 \u00d7 4\n\n8 \u00d7 8\n\nNormal ResNet-18\n0.7-Shallow\n\n99.58% 82.66% 17.56%\n97.36% 64.00% 11.94%\n\nTable 3: The accuracy by using normal and masked networks to classify randomly shuf\ufb02ed test images.\n\nFrom the results, we can see that our network with Random Mask always has lower accuracy than the normal\nnetwork on these randomly shuf\ufb02ed test images, which indeed accords with our intuition. By randomly\nshuf\ufb02ing the patches in images, we break the relative positions and margins of the objects and pose negative\nimpact to the network with Random Mask since it may rely on such information to classify. Note that\nrandomly shuf\ufb02ed images are surely dif\ufb01cult for humans to classify, so this experiment might also imply that\nthe network with Random Mask is more similar to human perception than the normal one.\n\nB ATTACK APPROACHES\n\nWe \ufb01rst give an overview of how to attack a neural network in some mathematical notations. Let x be the\ninput to the neural network and f\u03b8 be the function which represents the neural network with parameter \u03b8.\nThe output label of the network to the input can be computed as c = arg maxi f\u03b8(x). In order to perform\nan adversarial attack, we add a small perturbation \u03b4x to the original image and get an adversarial image\nxadv = x + \u03b4x. The new input xadv should look visually similar to the original x. Here we use the commonly\n\n12\n\n\fused (cid:96)\u221e-norm metric to measure similarity, i.e., we require that||\u03b4x|| \u2264 (cid:15). The attack is considered successful\nif the predicted label of the perturbed image cadv = arg maxi f\u03b8(xadv) is different from c.\n\nGenerally speaking, there are two types of attack methods: Targeted Attack, which aims to change the output\nlabel of an image to a speci\ufb01c (and different) one, and Untargeted Attack, which only aims to change the\noutput label and does not restrict which speci\ufb01c label the modi\ufb01ed example should let the network output.\n\nIn this paper, we mainly use the following three attack approaches. J denotes the loss function of the neural\nnetwork and y denotes the true label of x.\n\n\u2022 Fast Gradient Sign Method (FGSM). FGSM (Goodfellow et al., 2015) is a one-step untargeted\nmethod which generates the adversarial example xadv by adding the sign of the gradients multiplied\nby a step size (cid:15) to the original benign image x. Note that FGSM controls the (cid:96)\u221e-norm between the\nadversarial example and the original one by the parameter (cid:15).\n\nxadv = x + (cid:15) \u00b7 sign(\u2207xJ(x, y)).\n\n\u2022 Basic iterative method (PGD). PGD is a multiple-step attack method which applies FGSM multiple\ntimes. To make the adversarial example still stay \u201cclose\u201d to the original image, the image is projected\nto the (cid:96)\u221e-ball centered at the original image after every step. The radius of the (cid:96)\u221e-ball is called\nperturbation scale and is denoted by \u03b1.\n\nadv = x, xk+1\nx0\n\nadv = Clipx,\u03b1\n\nxk\n\nadv + (cid:15) \u00b7 sign(\u2207xk\n\nJ(xk\n\nadv, y))\n\nadv\n\n(cid:104)\n\n(cid:105)\n\n.\n\n\u2022 CW Attack. Carlini & Wagner (2016) shows that constructing an adversarial example can be\n\nformulated as solving the following optimization problem:\n\nxadv = arg min\n\nc \u00b7 g(x(cid:48)) + ||x(cid:48) \u2212 x||2\n2,\n\nx(cid:48)\n\nwhere c \u00b7 g(x(cid:48)) is the loss function that evaluates the quality of x(cid:48) as an adversarial example and the\nterm ||x(cid:48) \u2212 x||2\n2 controls the scale of the perturbation. More speci\ufb01cally, in the untargeted attack\nsetting, the loss function g(x) can be de\ufb01ned as:\n\ng(x) = max{max\ni(cid:54)=y\n\n(f (x)i) \u2212 f (x)y, \u2212\u03ba},\n\nwhere the parameter \u03ba is called con\ufb01dence.\n\nC NETWORK ARCHITECTURES\n\nHere we brie\ufb02y introduce the network architectures used in our experiments. Generally, we apply Random\nMask at the shallow layers of the networks and we have tried \ufb01ve different architectures, namely ResNet-18,\nResNet-50, DenseNet-121 SENet-18 and VGG-19. We next illustrate these architectures and show how we\napply Random Mask to them.\n\nC.1 RESNET-18\n\nResNet-18 (He et al., 2016) contains 5 blocks: the 0th block is one single 3 \u00d7 3 convolutional layer, and each\nof the rest contains four 3 \u00d7 3 convolutional layers. Figure 6 shows the whole structure of ResNet-18. In our\nexperiment, applying Random Mask to a block means applying Random Mask to every layer in it.\n\n13\n\n\fFigure 6: The architecture of ResNet-18\n\nC.2 RESNET-50\n\nSimilar to ResNet-18, ResNet-50 (He et al., 2016) contains 5 blocks and each block contains several 1 \u00d7 1\nand 3 \u00d7 3 convolutional layers (i.e. Bottlenecks). In our experiment, we apply Random Mask to the 3 \u00d7 3\nconvolutional layers in the \ufb01rst three \u201cshallow\u201d blocks. The masked layers in the 1st block are marked by the\nred arrows.\n\nFigure 7: The architecture of ResNet-50\n\nC.3 DENSENET-121\n\nDenseNet-121 (Huang et al., 2017) is another popular network architecture in deep learning researches. It\ncontains 5 Dense-Blocks, each of which contains several 1 \u00d7 1 and 3 \u00d7 3 convolutional layers. Similar to what\n\n14\n\n\fwe do for ResNet-50, we apply Random Mask to the 3 \u00d7 3 convolutional layers in the \ufb01rst three \u201cshallow\u201d\nblocks. The growth rate is set to 32 in our experiments.\n\nFigure 8: The architecture of DenseNet\n\nC.4 SENET\n\nSENet (Hu et al., 2017a), a network architecture which won the \ufb01rst place in ImageNet contest 2017, is\nshown in Figure 9. Note that here we use the pre-activation shortcut version of SENet and we apply Random\nMask to the convolutional layers in the \ufb01rst 3 SE-blocks.\n\nFigure 9: The architecture of SENet\n\nC.5 VGG-19\n\nVGG-19 (Simonyan & Zisserman, 2014) is a typical neural network architecture with sixteen 3 \u00d7 3 convolu-\ntional layers and three fully-connected layers. We slightly modi\ufb01ed the architecture by replacing the \ufb01nal 3\nfully connected layers with 1 fully connected layer as is suggested by recent architectures. We apply Random\nMask on the \ufb01rst four 3 \u00d7 3 convolutional layers.\n\n15\n\n\fFigure 10: The architecture of VGG-19\n\nD TRAINING PROCESS ON CIFAR-10 AND MNIST\n\nTo guarantee our experiments are reproducible, here we present more details on the training process in\nour experiments. When training models on CIFAR-10, we \ufb01rst subtract per-pixel mean. Then we apply a\nzero-padding of width 4, a random horizontal \ufb02ip and a random crop of size 32 \u00d7 32 on train data. No other\ndata augmentation method is used. We apply SGD with momentum parameter 0.9, weight decay parameter\n5 \u00d7 10\u22124 and mini-batch size 128 to train on the data for 350 epochs. The learning rate starts from 0.1 and is\ndivided by 10 when the number of epochs reaches 150 and 250. When training models on MNIST, we \ufb01rst\nsubtract per-pixel mean. Then we apply random horizontal \ufb02ip on train data. We apply SGD with momentum\nparameter 0.9, weight decay parameter 5 \u00d7 10\u22124 and mini-batch size 128 to train on the data for 50 epochs.\nThe learning rate starts from 0.1 and is divided by 10 when the number of epochs reaches 20 and 40. Figure 11\nshows the train and test curves of a normal ResNet-18 and a Random Masked ResNet-18 on CIFAR-10 and\nMNIST. Different network structures share similar tendency in terms of the train and test curves.\n\nCIFAR-10\n\nMNIST\n\nFigure 11: Train and test curve of normal ResNet-18 and Random Masked ResNet-18 on CIFAR-10 and\nMNIST.\n\n16\n\n\fE ADVERSARIAL EXAMPLES GENERATED BY APPLYING RANDOM MASK\n\nE.1 ADVERSARIAL EXAMPLES THAT CAN \u201cFOOL\u201d HUMAN\n\nFigure 12 shows some adversarial examples generated from CIFAR-10 along with the corresponding original\nimages. These examples are generated from CIFAR-10 against ResNet-18 with Random Mask of drop ratio\n0.8 on the 0th, 1st, 2nd blocks and another ResNet-18 with Random Mask of drop ratio 0.9 on the 1st, 2nd blocks.\nWe use attack method PGD with perturbation scale \u03b1 = 16 and \u03b1 = 32. We also show some adversarial\nexamples generated from Tiny-ImageNet1 along with the corresponding original images in Figure 13.\n\nDog\n\nBird\n\nFrog\n\nDog\n\nShip\n\nDog\n\nShip\n\nBird\n\nFrog\n\nBird\n\nAutomo-\nbile\n\nAirplane Airplane Truck\n\nHorse\n\nTruck Automo-\nbile\n\nBird\n\nAutomo-\nbile\n\nShip\n\nBird\n\nDeer\n\nFigure 12: The adversarial examples (upper) shown in Figure 1 along with the original images (lower) from\nCIFAR-10.\n\nMushroom Monarch But-\n\nLadybug\n\nBlack Widow Sulphur But-\n\nTeddy\n\nMushroom\n\nter\ufb02y\n\nter\ufb02y\n\nTarantula\n\nBlack Widow\n\nFly\n\nLadybug\n\nEgyptian Cat\n\nBrain Coral\n\nGold\ufb01sh\n\nFigure 13: Adversarial examples (upper) generated from Tiny-ImageNet against ResNet-18 with Random\nMask of ratio 0.9 on the 1st, 2nd blocks. along with the original images (lower). The attack methods are PGD\nwith scale 64 and 32, step size 1 and step number 40 and 80 respectively.\n\nE.2 RANDOMLY SELECTED ADVERSARIAL EXAMPLES\n\nSee Figure 15 for a randomly sampled set of images from Tiny-ImageNet along with the corresponding\nadversarial examples generated against ResNet-18 with Random Mask and normal ResNet-18.\n\n1https://tiny-imagenet.herokuapp.com/\n\n17\n\n\fF MORE EXPERIMENTAL RESULTS\n\nF.1 BLACK-BOX DEFENSE UNDER MADRY\u2019S SETTING\n\nHere we list the black-box settings in Madry\u2019s paper (Madry et al., 2017). In their experiments, ResNets are\ntrained by minimizing the following loss:\n\n(cid:20)\n\n(cid:21)\n\nE(x,y)\u223cD\n\nmin\n\u03b8\n\nmax\n\u03b4\u2208S\n\nL(\u03b8, x + \u03b4, y)\n\n.\n\nThe outer minimization is achieved by gradient descent and the inner maximization is achieved by generating\nPGD adversarial examples with step size 2, the number of steps 7 and the perturbation scale 8. After training,\nin their black-box attack setting, they generate adversarial examples from naturally trained neural networks\nand test them on their models. Both FGSM and PGD adversarial examples have step size or perturbation\nscale 8 and PGD runs for 7 gradient descent steps with step size 2.\n\nIn Table 1, we apply Random Mask to shallow blocks with drop ratio 0.85. The ratio is selected by considering\nthe trade-off of robustness and generalization performance, which is shown in Figure 14. When doing attacks,\nwe generate the adversarial examples in the same way as Madry\u2019s paper (Madry et al., 2017) does.\n\nFigure 14: Relationship between defense rate against adversarial examples generated by PGD and test\naccuracy with respect to different drop ratios under Madry\u2019s setting (Madry et al., 2017). Each red star\nrepresents a speci\ufb01c drop ratio with its value written near the star. We can see the trade-off between robustness\nand generalization.\n\n18\n\n\fF.2 BLACK-BOX DEFENSE OF SEVERAL NETWORK STRUCTURES ON CIFAR-10 AND MNIST\n\nIn this part, we apply Random Mask to \ufb01ve popular network structures - ResNet-18, ResNet-50, DenseNet-121,\nSENet-18, VGG-19, and test the black-box defense performance on CIFAR-10 and MNIST datasets.\n\nSince both the intuition (see Section 2) and the extensive experiments (see Section 3.3 and Appendix F.5)\nshow that we should apply Random Mask on the relatively shallow layers of the network structure, we would\nlike to do so in this part of experiments. Illustrations of Random Mask applied to these network structures\ncan be found in Appendix C. In addition, the detailed experiments on ResNet-18 (See Appendix F.5) show\nthat defense performances are consistent against adversarial examples generated under different settings.\nTherefore, for brevity, we evaluate the defense performance on adversarial examples generated by PGD only\nin this subsection.\n\nThe results can be found in Table 4 and Table 5. Networks in the leftmost column are the target models which\ndefend against adversarial examples. Networks in the \ufb01rst row are the source models to generate adversarial\nexamples by PGD. 0.5-shallow and 0.7-shallow mean applying Random Mask with drop ratio 0.5 and 0.7 to\nthe shallow layers of the network structure whose name lies just above them. The source and target networks\nare initialized differently if they share the same architecture. All the numbers except the Acc column mean\nthe success rate of defense. The numbers in the Acc column mean the classi\ufb01cation accuracy of the target\nmodel on clean test data. These results show that Random Mask can consistently improve the black-box\ndefense performance of different network structures.\n\nNetwork Structure\n\nResNet-18 ResNet-50 DenseNet-121\n\nSENet-18 VGG-19\n\nAcc\n\nNormal ResNet-18\n0.5-Shallow\n0.7-Shallow\nNormal ResNet-50\n0.5-Shallow\n0.7-Shallow\nNormal DenseNet-121\n0.5-Shallow\n0.7-Shallow\nNormal SENet-18\n0.5-Shallow\n0.7-Shallow\nNormal VGG-19\n0.5-Shallow\n0.7-Shallow\n\n1.42%\n18.46%\n39.61%\n4.33%\n16.79%\n32.69%\n4.10%\n12.67%\n23.79%\n1.10%\n17.47%\n34.19%\n6.94%\n29.73%\n49.94%\n\n0.42%\n12.36%\n33.73%\n0.10%\n5.24%\n19.56%\n0.58%\n4.18%\n14.54%\n0.52%\n11.39%\n27.27%\n4.23%\n25.86%\n45.74%\n\n2.96%\n26.50%\n47.76%\n3.95%\n20.48%\n38.34%\n0.60%\n10.89%\n25.44%\n2.29%\n23.69%\n43.13%\n9.98%\n38.66%\n57.74%\n\n1.38%\n21.42%\n44.95%\n3.93%\n19.70%\n37.58%\n3.24%\n13.30%\n27.90%\n0.62%\n19.62%\n39.05%\n6.76%\n35.80%\n56.17%\n\n9.88% 95.33%\n22.27% 93.39%\n39.66% 91.83%\n17.58% 95.25%\n24.82% 94.43%\n37.99% 93.46%\n10.11% 95.53%\n19.22% 93.97%\n29.61% 92.82%\n8.02% 95.09%\n22.66% 93.53%\n34.92% 92.54%\n4.63% 93.93%\n26.93% 91.73%\n46.10% 90.11%\n\nTable 4: Black-box experiments on CIFAR-10. Networks in the leftmost column are the target models\nwhich defend against adversarial examples. Networks in the \ufb01rst row are the source models to generate\nadversarial examples by PGD. PGD runs for 20 steps with step size 1 and perturbation scale 16. 0.5-shallow\nand 0.7-shallow mean applying Random Mask with drop ratio 0.5 and 0.7 to the shallow layers of the network\nstructure whose name lies just above them. All the numbers except the Acc column mean the success rate of\ndefense.\n\n19\n\n\fNetwork Structure ResNet-18 ResNet-50 DenseNet-121\n\nSENet-18 VGG-19\n\nAcc\n\nNormal ResNet-18\n0.5-Shallow\n0.7-Shallow\nNormal ResNet-50\n0.5-Shallow\n0.7-Shallow\nNormal ResNet-50\n0.5-Shallow\n0.7-Shallow\nNormal SENet-18\n0.5-Shallow\n0.7-Shallow\nNormal VGG-19\n0.5-Shallow\n0.7-Shallow\n\n0.06%\n3.45%\n15.28%\n3.99%\n9.78%\n9.90%\n1.24%\n3.09%\n3.59%\n0.52%\n3.33%\n8.84%\n4.09%\n9.54%\n21.00%\n\n13.80%\n17.18%\n47.68%\n4.91%\n11.93%\n17.13%\n18.10%\n18.34%\n36.63%\n14.31%\n16.85%\n26.97%\n33.19%\n33.97%\n37.68%\n\n2.34%\n7.41%\n19.29%\n5.79%\n9.90%\n10.47%\n0.04%\n1.52%\n2.18%\n3.17%\n5.83%\n9.36%\n6.25%\n9.12%\n20.92%\n\n0.10%\n4.49%\n21.05%\n3.09%\n9.09%\n9.27%\n1.83%\n3.99%\n5.78%\n0.08%\n1.73%\n8.97%\n6.36%\n13.90%\n26.44%\n\n8.69% 99.49%\n17.01% 99.34%\n38.54% 99.29%\n11.00% 99.39%\n14.11% 99.32%\n19.03% 99.28%\n10.36% 99.48%\n13.19% 99.46%\n20.76% 99.38%\n12.51% 99.41%\n13.67% 99.35%\n18.96% 99.32%\n2.45% 99.48%\n9.56% 99.37%\n30.10% 99.34%\n\nTable 5: Black-box experiments on MNIST. Networks in the leftmost column are the target models which\ndefend against adversarial examples. Networks in the \ufb01rst row are the source models to generate adversarial\nexamples by PGD. PGD runs for 40 steps with step size 0.01 \u00d7 255 and perturbation scale 0.3 \u00d7 255.\n0.5-shallow and 0.7-shallow mean applying Random Mask with drop ratio 0.5 and 0.7 to the shallow layers\nof the network structure whose name lies just above them. All the numbers except the Acc column mean the\nsuccess rate of defense.\n\nF.3 WHITE-BOX\n\nSee Table 6 for the defense performance of ResNet-18 with Random Mask against white-box attacks on\nCIFAR-10 dataset. All the numbers except the Acc column mean the success rate of defense. The results on\nother network architectures are similar.\n\nNetwork Structure FGSM1\nNormal ResNet-18\n0.5-Shallow\n0.7-Shallow\n\nPGD8\nPGD4\nPGD2\n3.40% 0.02% 95.33%\n81.24% 65.78% 51.24% 24.26%\n9.11% 0.42% 93.39%\n85.22% 68.65% 52.04% 42.35%\n85.70% 69.69% 54.51% 49.30% 19.88% 3.28% 91.83%\n\nFGSM2\n\nFGSM4\n\nAcc\n\nTable 6: White-box defense performance. FGSM1, FGSM2, FGSM4 refer to FGSM with step size 1,2,4\nrespectively. PGD2, PGD4, PGD8 refer to PGD with perturbation scale 2,4,8 and step number 4,6,10 respec-\ntively. The step size of all PGD are set to 1.\n\nF.4 TRANSFERABILITY AND GRAY-BOX DEFENSE\n\nHere we show the gray-box defense ability of Random Mask and the transferability of the adversarial examples\ngenerated against Random Mask on CIFAR-10 dataset. We generate gray-box attacks in the following two\nways. One way is to generate adversarial examples against one trained neural network and test those images\non a network with the same structure but different initialization. The other way is speci\ufb01c to our Random\nMask models. We generate adversarial examples on one trained network with Random Mask and test them\non a network with the same drop ratio but different Random Mask. In both of these two ways, the adversarial\n\n20\n\n\fknows some information on the structure of the network, but does not know the parameters of it. To see the\ntransferability of the generated adversarial examples, we also test them on DenseNet-121 and VGG-19.\n\nSource\n\nTarget\nNormal ResNet-18\n0.5-Shallow\n0.5-ShallowDIF\n0.7-Shallow\n0.7-ShallowDIF\nNormal DenseNet-121\nNormal VGG-19\n\nNormal ResNet-18\n\n0.5-Shallow 0.7-Shallow\n\n13.91%\n28.83%\n28.91%\n49.14%\n48.23%\n20.23%\n15.83%\n\n20.90%\n20.22%\n19.37%\n31.30%\n31.77%\n22.38%\n17.43%\n\n28.42%\n23.24%\n22.11%\n23.26%\n23.54%\n28.05%\n20.85%\n\nTable 7: Results on gray-box attacks and transferability. We use FGSM with step size 16 to generate the\nadversarial examples on source networks and test them on target networks. For target networks, Normal\nResNet-18, 0.5-Shallow and 0.7-Shallow represent the networks with the same structure as the corresponding\nsource networks but with different initialization values. 0.5-ShallowDIF and 0.7-ShallowDIF represent the\nnetworks with the same drop ratios as the corresponding source networks but with different random masks.\n\nTable 7 shows that Random Mask can also improve the performance under gray-box attacks. In addition, we\n\ufb01nd that CNNs with Random Mask have similar performance on adversarial examples generated by our two\nkinds of gray-box attacks. This phenomenon indicates that CNNs with Random Mask of same ratios have\nsimilar properties and catch similar information.\n\nF.5 FULL INFORMATION ON EXPERIMENTS MENTIONED IN SECTION 3.3\n\nIn this part, we will show more experimental results on Random Mask using different adversarial examples,\ndifferent attack methods and different mask settings on ResNet-18. More speci\ufb01cally, we choose 5000 test\nimages from CIFAR-10 which are correctly classi\ufb01ed by the original network to generate FGSM and PGD\nadversarial examples, and 1000 test images for CW attack.\n\nFor FGSM, we try step size (cid:15) \u2208 {8, 16, 32}, namely FGSM8, FGSM16, FGSM32, to generate adver-\nsarial examples. For PGD, we have tried more extensive settings. Let {(cid:15), T, \u03b1} be the PGD setting\nwith step size (cid:15), the number of steps T and the perturbation scale \u03b1, then we have tried PGD set-\n(1, 8, 4), (2, 4, 4), (4, 2, 4), (1, 12, 8), (2, 6, 8), (4, 3, 8), (1, 20, 16), (2, 10, 16), (4, 5, 16), (1, 40, 32),\ntings\n(2, 20, 32), (4, 10, 32) to generate PGD adversarial examples. From the experimental results, we observe the\nfollowing phenomena. First, we \ufb01nd that the larger the perturbation scale is, the stronger the adversarial\nexamples are. Second, for a \ufb01xed perturbation scale, the smaller the step size is, the more successful the\nattack is, as it searches the adversarial examples in a more careful way around the original image. Based on\nthese observation, we only show strong PGD attack results in the Appendix, namely the settings (1, 20, 16)\n(PGD16), (2, 10, 16) (PGD2,16) and (1, 40, 32) (PGD32). Nonetheless, our models also perform much better\non weak PGD attacks. For CW attack, we have also tried different con\ufb01dence parameters \u03ba. However, we\n\ufb01nd that for large \u03ba, the algorithm is hard to \ufb01nd adversarial examples for some neural networks such as\nVGG because of its logit scale. For smaller \u03ba, the adversarial examples have weak transfer ability, which\nmeans they can be easily defensed even by normal networks. Therefore, in order to balance these two factors,\nwe choose \u03ba = 40 (CW40) for DenseNet-121, ResNet-50, SENet-18 and \u03ba = 20 (CW20) for ResNet-18 as a\ngood choice to compare our models with normal ones. The step number for choosing the parameter c is set to\n30.\n\n21\n\n\fNote that the noise of FGSM and PGD is considered in the sense of (cid:96)\u221e norm and the noise of CW is\nconsidered in the sense of (cid:96)2 norm. All adversarial examples used to evaluate can fool the original network.\nTable 8,9,10,11 and 12 list our experimental results. DC means we replace Random Mask with a decreased\nnumber of channels in the corresponding blocks to achieve the same drop ratio. SM means we use the same\nmask on all the channels in a layer. \u00d7n means we multiply the number of the channels in the corresponding\nblocks by n times. EN means we ensemble \ufb01ve models with different masks of the same drop ratio.\n\nNetwork\n\nNormal ResNet-18\n0.3-Shallow\n0.5-Shallow\n0.7-Shallow\n0.75-Shallow\n0.8-Shallow\n0.85-Shallow\n0.9-Shallow\n0.95-Shallow\n0.3-Deep\n0.5-Deep\n0.7-Deep\n0.5-Shallow, 0.5-Deep\n0.3-Shallow, 0.3-Deep\n0.3-Shallow, 0.7-Deep\n0.7-Shallow, 0.7-Deep\n0.7-Shallow, 0.3-Deep\n0.5-ShallowDC\n0.7-ShallowDC\n0.9-ShallowDC\n0.5-ShallowSM\n0.7-ShallowSM\n0.9-ShallowSM\n0.5-Shallow\u00d72\n0.7-Shallow\u00d72\n0.9-Shallow\u00d72\n0.9-Shallow\u00d74\nNormal ResNet-18EN\n0.5-Shallow\u00d72,EN\n0.5-ShallowEN\n0.7-ShallowEN\n0.85-ShallowEN\n0.9-ShallowEN\n\nFGSM8\n29.78%\n55.40%\n66.87%\n79.50%\n83.12%\n85.49%\n88.18%\n94.08%\n96.16%\n28.51%\n25.01%\n23.94%\n58.49%\n51.03%\n36.16%\n64.85%\n74.73%\n36.39%\n43.81%\n49.53%\n77.30%\n82.59%\n67.06%\n51.25%\n68.82%\n88.00%\n82.96%\n35.89%\n55.25%\n69.35%\n81.98%\n90.13%\n95.37%\n\nFGSM16\n14.91%\n23.29%\n30.86%\n48.57%\n59.22%\n63.01%\n65.27%\n79.93%\n87.36%\n14.62%\n10.76%\n11.23%\n26.34%\n24.15%\n11.26%\n27.43%\n40.58%\n12.15%\n17.74%\n19.00%\n48.86%\n48.03%\n39.40%\n20.78%\n30.94%\n68.83%\n59.64%\n16.24%\n19.84%\n31.38%\n51.81%\n66.51%\n81.95%\n\n9.16%\n\nPGD16\nFGSM32\n11.53%\n2.96%\n7.73% 14.53%\n6.65% 26.50%\n10.51% 47.76%\n17.16% 56.08%\n15.57% 63.16%\n18.33% 69.40%\n43.70% 83.08%\n59.05% 89.98%\n1.95%\n8.78%\n2.57%\n10.24%\n10.48%\n3.24%\n11.08% 19.36%\n10.82% 12.67%\n7.94%\n10.09% 32.72%\n9.12% 42.95%\n4.68%\n8.24%\n8.32%\n7.51%\n7.23% 19.33%\n12.50% 44.04%\n12.30% 57.62%\n16.25% 50.40%\n10.29% 12.51%\n7.22% 29.83%\n28.55% 66.86%\n19.44% 59.15%\n9.92%\n2.22%\n8.36% 11.86%\n7.73% 27.58%\n8.57% 50.79%\n17.91% 71.38%\n43.42% 85.14%\n\nAcc\n\n2.43%\n3.81%\n4.07%\n20.77%\n14.06%\n8.00%\n\nCW40\nPGD32\nPGD2,16\n2.26%\n8.23% 95.33%\n3.44%\n16.00%\n5.73% 36.95% 94.03%\n28.65% 10.33% 54.02% 93.39%\n49.62% 21.39% 73.70% 91.83%\n58.81% 26.87% 77.82% 91.46%\n65.16% 32.86% 81.75% 91.18%\n71.21% 36.12% 85.46% 90.15%\n83.72% 55.02% 89.67% 87.68%\n90.13% 68.25% 90.24% 84.53%\n7.88% 95.16%\n1.30%\n4.52%\n7.19% 94.94%\n2.64% 10.10% 94.61%\n9.07% 43.59% 92.39%\n6.75% 29.65% 94.16%\n5.77% 23.31% 93.44%\n33.84% 16.26% 62.47% 89.78%\n45.32% 19.00% 68.58% 91.23%\n4.05% 12.72% 94.97%\n5.52%\n9.20%\n4.52% 19.34% 94.23%\n20.88% 10.08% 44.80% 93.27%\n46.58% 19.81% 72.07% 92.57%\n57.83% 24.81% 79.55% 89.81%\n50.20% 29.23% 65.38% 74.28%\n13.89%\n5.38% 34.00% 94.12%\n31.74% 11.44% 60.17% 93.01%\n69.42% 37.51% 82.74% 90.49%\n60.72% 32.29% 78.88% 90.57%\n1.46%\n2.56%\n8.58% 96.12%\n5.30% 37.37% 95.24%\n13.44%\n29.68%\n9.97% 58.07% 94.56%\n53.88% 23.28% 77.74% 93.31%\n72.30% 38.25% 87.37% 91.77%\n85.79% 56.02% 91.36% 89.45%\n\nExtended experimental\nThe model\n\nTable 8:\nresults of Section 3.3.\nAdversarial examples generated\nagainst DenseNet-121.\ntrained on CIFAR-10 achieves 95.62% accuracy on test set.\n\u03c3-ShallowDC, \u03c3-ShallowSM, \u03c3-Shallow\u00d7n and \u03c3-ShallowEN mean dropping channels with ratio \u03c3, apply-\ning same mask with ratio \u03c3, increasing channel number to n times with mask ratio \u03c3 for every channel and\nensemble \ufb01ve models with different masks of same ratio \u03c3 respectively. The entries in the middle seven\ncolumns are success rates of defense under different settings.\n\n22\n\n\fNetwork\n\nNormal ResNet-18\n0.3-Shallow\n0.5-Shallow\n0.7-Shallow\n0.75-Shallow\n0.8-Shallow\n0.85-Shallow\n0.9-Shallow\n0.95-Shallow\n0.3-Deep\n0.5-Deep\n0.7-Deep\n0.5-Shallow, 0.5-Deep\n0.3-Shallow, 0.3-Deep\n0.3-Shallow, 0.7-Deep\n0.7-Shallow, 0.7-Deep\n0.7-Shallow, 0.3-Deep\n0.5-ShallowDC\n0.7-ShallowDC\n0.9-ShallowDC\n0.5-ShallowSM\n0.7-ShallowSM\n0.9-ShallowSM\n0.5-Shallow\u00d72\n0.7-Shallow\u00d72\n0.9-Shallow\u00d72\n0.9-Shallow\u00d74\nNormal ResNet-18EN\n0.5-Shallow\u00d72,EN\n0.5-ShallowEN\n0.7-ShallowEN\n0.85-ShallowEN\n0.9-ShallowEN\n\nFGSM8\n26.99%\n48.76%\n59.66%\n74.00%\n78.37%\n81.67%\n86.31%\n92.89%\n95.07%\n25.96%\n25.21%\n24.36%\n53.46%\n43.32%\n34.09%\n61.22%\n70.43%\n32.86%\n37.96%\n48.54%\n73.96%\n80.80%\n69.15%\n46.50%\n63.37%\n84.28%\n78.24%\n29.66%\n49.16%\n63.38%\n77.25%\n88.56%\n94.31%\n\nFGSM16\n13.91%\n21.32%\n30.48%\n47.11%\n56.05%\n59.14%\n63.16%\n77.90%\n85.40%\n15.46%\n9.21%\n9.49%\n24.65%\n20.55%\n11.05%\n28.11%\n39.15%\n13.89%\n16.23%\n19.10%\n47.63%\n48.37%\n43.55%\n21.37%\n29.90%\n64.47%\n56.31%\n14.37%\n19.81%\n30.25%\n50.07%\n65.23%\n79.47%\n\nPGD16\nFGSM32\n1.42%\n3.57%\n8.14%\n9.54%\n11.60% 18.46%\n15.65% 39.61%\n21.44% 49.14%\n19.60% 55.84%\n22.23% 64.73%\n45.63% 81.70%\n59.91% 88.31%\n1.18%\n7.18%\n2.17%\n1.44%\n2.60%\n2.36%\n7.08% 13.48%\n7.31%\n4.14%\n6.01%\n1.58%\n13.78% 27.12%\n13.94% 36.88%\n1.93%\n3.71%\n4.30%\n5.05%\n11.37% 14.34%\n16.60% 36.19%\n15.26% 53.69%\n20.26% 50.68%\n6.86%\n6.06%\n12.07% 20.70%\n31.90% 62.65%\n23.28% 52.38%\n0.98%\n3.97%\n7.02%\n6.73%\n11.05% 18.15%\n13.80% 41.59%\n22.50% 65.68%\n44.67% 82.97%\n\nAcc\n\n1.28%\n2.63%\n3.08%\n15.65%\n9.36%\n6.77%\n\nCW20\nPGD32\nPGD2,16\n0.96%\n2.19% 95.33%\n1.84%\n4.02% 38.87% 94.03%\n9.51%\n21.44%\n7.70% 60.65% 93.39%\n43.17% 16.09% 79.04% 91.83%\n52.21% 20.31% 81.59% 91.46%\n59.63% 26.61% 82.78% 91.18%\n67.03% 31.33% 86.06% 90.15%\n82.50% 54.12% 90.29% 87.68%\n89.64% 66.52% 90.97% 84.53%\n2.66% 95.16%\n0.88%\n3.15% 94.94%\n2.31%\n1.31%\n6.62% 94.61%\n6.99% 49.95% 92.39%\n4.46% 32.92% 94.16%\n4.56% 24.11% 93.44%\n30.24% 13.51% 69.15% 89.78%\n39.81% 15.45% 74.57% 91.23%\n2.19%\n6.10% 94.97%\n2.89%\n2.65% 15.44% 94.23%\n5.96%\n16.01%\n7.04% 50.62% 93.27%\n40.86% 15.52% 73.68% 92.57%\n54.78% 22.90% 82.34% 89.81%\n50.80% 28.82% 71.62% 74.28%\n3.59% 39.12% 94.12%\n8.65%\n7.76% 67.02% 93.01%\n24.08%\n65.24% 33.06% 85.08% 90.49%\n55.63% 25.91% 82.26% 90.57%\n0.54%\n1.24%\n2.00% 96.12%\n8.50%\n3.46% 38.12% 95.24 %\n6.71% 63.90% 94.56%\n21.14%\n44.78% 15.85% 80.86% 93.31%\n68.31% 32.77% 88.26% 91.77%\n89.4 %\n84.05% 54.46% 90.52%\n\nresults of Section 3.3.\nAdversarial examples are generated\nTable 9: Extended experimental\nagainst ResNet-18.\ntrained on CIFAR-10 achieves 95.27% accuracy on test set.\nThe model\n\u03c3-ShallowDC, \u03c3-ShallowSM, \u03c3-Shallow\u00d7n and \u03c3-ShallowEN mean dropping channels with ratio \u03c3, apply-\ning same mask with ratio \u03c3, increasing channel number to n times with mask ratio \u03c3 for every channel and\nensemble \ufb01ve models with different masks of same ratio \u03c3 respectively. The entries in the middle seven\ncolumns are success rates of defense under different settings.\n\n23\n\n\fNetwork\n\nNormal ResNet-18\n0.3-Shallow\n0.5-Shallow\n0.7-Shallow\n0.75-Shallow\n0.8-Shallow\n0.85-Shallow\n0.9-Shallow\n0.95-Shallow\n0.3-Deep\n0.5-Deep\n0.7-Deep\n0.5-Shallow, 0.5-Deep\n0.3-Shallow, 0.3-Deep\n0.3-Shallow, 0.7-Deep\n0.7-Shallow, 0.7-Deep\n0.7-Shallow, 0.3-Deep\n0.5-ShallowDC\n0.7-ShallowDC\n0.9-ShallowDC\n0.5-ShallowSM\n0.7-ShallowSM\n0.9-ShallowSM\n0.5-Shallow\u00d72\n0.7-Shallow\u00d72\n0.9-Shallow\u00d72\n0.9-Shallow\u00d74\nNormal ResNet-18EN\n0.5-Shallow\u00d72,EN\n0.5-ShallowEN\n0.7-ShallowEN\n0.85-ShallowEN\n0.9-shallowen\n\nFGSM8\n29.33%\n45.32%\n56.26%\n70.57%\n77.18%\n80.33%\n84.81%\n92.17%\n94.43%\n27.78%\n27.24%\n24.81%\n48.78%\n42.18%\n33.11%\n56.39%\n66.33%\n31.56%\n37.52%\n44.00%\n69.40%\n77.25%\n64.32%\n41.51%\n58.59%\n83.05%\n75.74%\n32.70%\n44.90%\n59.64%\n73.45%\n87.58%\n93.87%\n\nFGSM16\n15.14%\n18.89%\n27.32%\n42.40%\n53.01%\n56.21%\n61.02%\n77.68%\n85.54%\n15.03%\n10.29%\n9.99%\n21.04%\n18.20%\n11.08%\n24.14%\n36.31%\n13.64%\n15.72%\n16.90%\n41.82%\n44.94%\n39.76%\n18.47%\n25.92%\n63.73%\n55.03%\n15.49%\n16.55%\n26.21%\n45.60%\n62.24%\n79.15%\n\n8.07%\n2.47%\n2.50%\n6.66%\n5.26%\n2.27%\n\nPGD16\nFGSM32\n0.42%\n3.88%\n4.36%\n9.16%\n10.72% 12.36%\n14.98% 33.73%\n19.68% 42.67%\n18.03% 52.45%\n21.50% 62.62%\n45.93% 81.20%\n60.71% 88.63%\n0.42%\n0.52%\n0.67%\n7.51%\n3.96%\n2.45%\n12.18% 21.86%\n13.09% 30.13%\n0.80%\n4.87%\n1.79%\n5.38%\n10.30%\n8.93%\n14.27% 29.83%\n13.80% 49.52%\n19.21% 50.16%\n3.67%\n6.02%\n11.20% 14.75%\n29.22% 58.85%\n21.59% 48.93%\n0.32%\n4.93%\n2.70%\n6.41%\n10.17% 11.41%\n12.99% 33.62%\n21.81% 62.84%\n46.44% 82.71%\n\nAcc\n\nPGD32\n0.08%\n0.92%\n3.48%\n\nCW40\nPGD2,16\n0.00% 95.33%\n0.96%\n1.98% 94.03%\n5.81%\n15.29%\n8.92% 93.39%\n37.56% 12.38% 33.08% 91.83%\n46.72% 15.88% 39.10% 91.46%\n55.54% 22.17% 47.52% 91.18%\n63.80% 29.35% 53.71% 90.15%\n82.50% 53.44% 66.70% 87.68%\n89.16% 66.69% 71.82% 84.53%\n0.00% 95.16%\n0.12%\n0.60%\n0.00% 94.94%\n0.50%\n1.10%\n0.00% 94.61%\n0.20%\n1.01%\n5.09% 92.39%\n2.52%\n9.79%\n1.64% 94.16%\n1.65%\n5.44%\n1.29%\n0.55% 93.44%\n3.54%\n9.01% 22.25% 89.78%\n24.88%\n33.96% 12.09% 30.68% 91.23%\n1.30%\n0.11% 94.97%\n0.28%\n2.91%\n0.44% 94.23%\n0.56%\n11.35%\n3.46%\n4.95% 93.27%\n9.60% 26.65% 92.57%\n33.51%\n51.13% 21.24% 46.44% 89.81%\n49.15% 28.24% 45.18% 74.28%\n0.62%\n4.80%\n1.32% 94.12%\n4.34% 13.77% 93.01%\n18.29%\n61.82% 28.09% 50.11% 90.49%\n51.78% 20.52% 47.08% 90.57%\n0.00% 96.12%\n0.84%\n4.00%\n1.42% 95.24%\n8.12% 94.56%\n14.23%\n38.49% 12.06% 32.60% 93.31%\n64.88% 29.10% 54.29% 91.77%\n83.32% 53.87% 67.71% 89.45%\n\n0.06%\n0.64%\n2.48%\n\nThe model\n\nAdversarial examples are generated\nTable 10: Extended experimental results of Section 3.3.\nagainst ResNet-50.\ntrained on CIFAR-10 achieves 95.69% accuracy on test set.\n\u03c3-ShallowDC, \u03c3-ShallowSM, \u03c3-Shallow\u00d7n and \u03c3-ShallowEN mean dropping channels with ratio \u03c3, apply-\ning same mask with ratio \u03c3, increasing channel number to n times with mask ratio \u03c3 for every channel and\nensemble \ufb01ve models with different masks of same ratio \u03c3 respectively. The entries in the middle seven\ncolumns are success rates of defense under different settings.\n\n24\n\n\fNetwork\n\nNormal ResNet-18\n0.3-Shallow\n0.5-Shallow\n0.7-Shallow\n0.75-Shallow\n0.8-Shallow\n0.85-Shallow\n0.9-Shallow\n0.95-Shallow\n0.3-Deep\n0.5-Deep\n0.7-Deep\n0.5-Shallow, 0.5-Deep\n0.3-Shallow, 0.3-Deep\n0.3-Shallow, 0.7-Deep\n0.7-Shallow, 0.7-Deep\n0.7-Shallow, 0.3-Deep\n0.5-ShallowDC\n0.7-ShallowDC\n0.9-ShallowDC\n0.5-ShallowSM\n0.7-ShallowSM\n0.9-ShallowSM\n0.5-Shallow\u00d72\n0.7-Shallow\u00d72\n0.9-Shallow\u00d72\n0.9-Shallow\u00d74\nNormal ResNet-18EN\n0.5-Shallow\u00d72,EN\n0.5-ShallowEN\n0.7-ShallowEN\n0.85-ShallowEN\n0.9-ShallowEN\n\nFGSM8\n25.53%\n46.12%\n57.05%\n72.67%\n78.23%\n82.27%\n85.80%\n92.93%\n94.77%\n23.76%\n23.01%\n22.87%\n51.20%\n42.34%\n31.43%\n57.26%\n68.66%\n30.81%\n34.57%\n43.46%\n71.27%\n79.48%\n65.85%\n44.13%\n60.51%\n85.26%\n78.65%\n27.87%\n45.04%\n60.42%\n76.08%\n88.64%\n94.40%\n\nFGSM16\n17.47%\n23.30%\n31.01%\n48.17%\n58.19%\n61.61%\n65.92%\n79.13%\n87.13%\n16.66%\n12.19%\n11.61%\n25.49%\n22.07%\n12.17%\n29.36%\n41.32%\n14.77%\n17.32%\n17.61%\n49.21%\n49.66%\n42.59%\n21.71%\n30.89%\n67.91%\n60.05%\n17.80%\n20.25%\n31.08%\n51.49%\n67.26%\n81.32%\n\n7.84%\n6.34%\n\n9.52%\n6.56%\n6.63%\n\nPGD16\nFGSM32\n1.38%\n8.56%\n9.57%\n10.48%\n11.07% 21.42%\n15.20% 44.95%\n21.20% 53.56%\n19.70% 61.55%\n22.73% 69.82%\n48.34% 84.55%\n63.36% 90.63%\n1.12%\n1.73%\n2.12%\n10.43% 15.05%\n8.47%\n6.19%\n13.99% 31.68%\n13.72% 40.56%\n2.13%\n6.08%\n3.65%\n8.04%\n10.54% 15.15%\n16.27% 41.34%\n15.65% 58.96%\n21.87% 52.63%\n8.91%\n9.49%\n11.58% 24.82%\n32.51% 67.34%\n24.45% 57.88%\n1.02%\n8.81%\n7.46%\n9.69%\n10.77% 20.98%\n13.19% 46.98%\n23.30% 71.44%\n48.52% 85.81%\n\nAcc\n\n1.26%\n2.05%\n2.54%\n17.41%\n9.68%\n6.25%\n\nCW40\nPGD32\nPGD2,16\n0.00% 95.33%\n0.84%\n1.78%\n2.66% 94.03%\n4.44%\n10.19%\n23.17%\n7.90% 14.61% 93.39%\n46.90% 19.67% 39.89% 91.83%\n55.32% 23.24% 47.33% 91.46%\n63.83% 31.05% 51.14% 91.18%\n70.55% 34.95% 57.36% 90.15%\n84.63% 55.58% 65.63% 87.68%\n90.61% 69.07% 69.14% 84.53%\n0.00% 95.16%\n0.66%\n0.00% 94.94%\n2.05%\n1.29%\n0.19% 94.61%\n7.43% 12.26% 92.39%\n1.70% 94.16%\n4.44%\n1.53% 93.44%\n4.82%\n32.58% 15.16% 30.00% 89.78%\n42.30% 17.58% 35.71% 91.23%\n0.00% 94.97%\n2.61%\n0.19% 94.23%\n4.54%\n15.51%\n7.41% 93.27%\n43.00% 16.93% 34.92% 92.57%\n60.00% 26.78% 48.28% 89.81%\n52.91% 30.38% 43.22% 74.28%\n3.47%\n9.61%\n2.65% 94.12%\n8.36% 21.52% 93.01%\n26.79%\n69.14% 36.78% 52.96% 90.49%\n59.63% 29.79% 50.19% 90.57%\n0.00% 96.12%\n1.24%\n0.52%\n9.02%\n2.84% 95.24%\n3.30%\n7.09% 13.61% 94.56%\n22.56%\n49.31% 18.95% 39.51% 93.31%\n72.42% 37.16% 56.36% 91.77%\n86.28% 56.14% 66.99% 89.45%\n\n1.83%\n2.07%\n7.12%\n\nThe model\n\nAdversarial examples are generated\nTable 11: Extended experimental results of Section 3.3.\nagainst SENet-18.\ntrained on CIFAR-10 achieves 95.15% accuracy on test set.\n\u03c3-ShallowDC, \u03c3-ShallowSM, \u03c3-Shallow\u00d7n and \u03c3-ShallowEN mean dropping channels with ratio \u03c3, apply-\ning same mask with ratio \u03c3, increasing channel number to n times with mask ratio \u03c3 for every channel and\nensemble \ufb01ve models with different masks of same ratio \u03c3 respectively. The entries in the middle seven\ncolumns are success rates of defense under different settings.\n\n25\n\n\fNetwork\n\nNormal ResNet-18\n0.3-Shallow\n0.5-Shallow\n0.7-Shallow\n0.75-Shallow\n0.8-Shallow\n0.85-Shallow\n0.9-Shallow\n0.95-Shallow\n0.3-Deep\n0.5-Deep\n0.7-Deep\n0.5-Shallow, 0.5-Deep\n0.3-Shallow, 0.3-Deep\n0.3-Shallow, 0.7-Deep\n0.7-Shallow, 0.7-Deep\n0.7-Shallow, 0.3-Deep\n0.5-ShallowDC\n0.7-ShallowDC\n0.9-ShallowDC\n0.5-ShallowSM\n0.7-ShallowSM\n0.9-ShallowSM\n0.5-Shallow\u00d72\n0.7-Shallow\u00d72\n0.9-Shallow\u00d72\n0.9-Shallow\u00d74\nNormal ResNet-18EN\n0.5-Shallow\u00d72,EN\n0.5-ShallowEN\n0.7-ShallowEN\n0.85-ShallowEN\n0.9-ShallowEN\n\nFGSM8\n37.67%\n50.06%\n57.35%\n71.75%\n76.81%\n79.46%\n85.51%\n92.58%\n95.24%\n36.72%\n35.93%\n34.05%\n52.05%\n47.36%\n40.38%\n59.19%\n67.14%\n37.37%\n42.39%\n47.41%\n69.61%\n79.69%\n67.77%\n46.93%\n60.23%\n83.32%\n77.68%\n39.42%\n49.51%\n60.43%\n74.11%\n87.48%\n94.14%\n\nFGSM16\n20.25%\n23.54%\n30.52%\n47.35%\n56.69%\n61.45%\n66.55%\n80.68%\n87.10%\n18.97%\n13.80%\n13.06%\n24.88%\n22.74%\n13.50%\n28.00%\n40.57%\n16.99%\n19.90%\n21.12%\n46.57%\n48.86%\n44.38%\n21.74%\n29.72%\n66.44%\n58.73%\n19.53%\n19.29%\n31.07%\n50.89%\n67.75%\n82.46%\n\nPGD16\nFGSM32\n5.40%\n9.88%\n9.53% 14.99%\n11.13% 22.27%\n15.47% 39.66%\n19.44% 47.88%\n21.36% 56.21%\n25.35% 64.09%\n51.90% 81.65%\n64.22% 88.50%\n9.65%\n9.21%\n2.99% 10.24%\n4.04% 10.36%\n6.74% 17.83%\n5.04% 14.44%\n3.28% 11.38%\n12.13% 29.88%\n13.80% 37.63%\n6.62%\n9.76%\n6.74% 11.93%\n11.43% 20.10%\n14.85% 37.31%\n13.87% 52.11%\n20.74% 49.64%\n7.11% 14.52%\n11.07% 23.52%\n33.11% 61.73%\n24.40% 52.07%\n6.69%\n8.67%\n7.16% 13.71%\n10.50% 21.54%\n13.54% 41.26%\n25.62% 65.68%\n52.59% 83.05%\n\nAcc\n\nPGD32\nPGD2,16\n6.72% 95.33%\n12.81%\n9.28% 94.03%\n18.66%\n26.87% 10.85% 93.39%\n44.42% 15.91% 91.83%\n53.76% 18.76% 91.46%\n61.01% 23.16% 91.18%\n67.77% 27.53% 90.15%\n83.90% 52.47% 87.68%\n90.04% 64.64% 84.53%\n6.40% 95.16%\n11.48%\n8.95% 94.94%\n12.77%\n12.70%\n7.46% 94.61%\n21.75% 10.37% 92.39%\n9.08% 94.16%\n17.82%\n14.52%\n8.12% 93.44%\n34.28% 14.12% 89.78%\n42.54% 16.00% 91.23%\n8.07% 94.97%\n12.35%\n15.39%\n8.01% 94.23%\n23.16% 10.82% 93.27%\n43.96% 15.26% 92.57%\n56.12% 20.35% 89.81%\n51.28% 27.31% 74.28%\n18.17%\n8.09% 94.12%\n28.11% 11.56% 93.01%\n66.76% 30.33% 90.49%\n57.72% 23.55% 90.57%\n6.46% 96.12%\n12.13%\n17.73%\n8.74% 95.24%\n27.09% 10.99% 94.56%\n47.68% 16.01% 93.31%\n69.68% 29.26% 91.77%\n85.46% 53.42% 89.45%\n\nExtended experimental\nThe model\n\nresults of Section 3.3.\nAdversarial examples are gener-\nTable 12:\nated against VGG-19.\ntrained on CIFAR-10 achieves 94.04% accuracy on test set.\n\u03c3-ShallowDC, \u03c3-ShallowSM, \u03c3-Shallow\u00d7n and \u03c3-ShallowEN mean dropping channels with ratio \u03c3, apply-\ning same mask with ratio \u03c3, increasing channel number to n times with mask ratio \u03c3 for every channel and\nensemble \ufb01ve models with different masks of same ratio \u03c3 respectively. The entries in the middle six columns\nare success rates of defense under different settings.\n\n26\n\n\fFigure 15: Randomly sampled images from Tiny-ImageNet dataset. The network structure used to generate\nthese images is ResNet-18 with Random Mask of ratio 0.9 on the 1st, 2nd blocks. The attack method is PGD\nwith perturbation scale 64, step size 1 and step number 80. For each image, we show the image generated\nagainst network with Random Mask (upper), the image generated against the normal ResNet-18 (middle) and\nthe original image (lower).\n\n27\n\n\f",
    "review_0": {
        "confidence": "3: The reviewer is fairly confident that the evaluation is correct",
        "rating": "4: Ok but not good enough - rejection",
        "review": "I am upgrading my reviews after the rebuttal, which actually has convinced me that there is something interesting going on in this paper. However, I'm not entirely convinced as the approach seems to be ad hoc. the intuitions provided are somewhat satisfactory, but it's not clear why the method works.. for example, the approach is highly sensitive to the hyperparameter \"drop rate\" and there is no way to find a good value for it. I'm inclined towards rejection as, even though results are almost satisfying, I yet don't understand what exactly is happening. Most of the arguments seems to be handwavy. I personally feel like a paper as simple as this one with not enough conceptual justifications, but good results (like this one), should go to a workshop. \n\n======\nThe authors propose to randomly drop a few parameters at the beginning and fix the resulting architecture for train and test. The claim is that the resulting network is robust to adversarial attacks.\n\nMajor concerns:\nAn extremely simple approach of pruning neural networks (randomly dropping weights) with no justification whatsoever. There are so many other network pruning papers available. If the point is to use pruned network then the authors must provide analysis over other pruning schemes as well.\n\nAnother major concern (technical contributions): How is the idea of randomly dropping weights different from Deep Expander Networks (Prabhu et al., ECCV 2018)? Please clarify.\n\nMinor suggestion: Another simple approach to test the hypotheses would be to try dropout at test time and see the performance.",
        "title": "Simple approach with experimental validations, however, seems ad hoc"
    },
    "review_1": {
        "confidence": "3: The reviewer is fairly confident that the evaluation is correct",
        "rating": "6: Marginally above acceptance threshold",
        "review": "This paper proposes a surprisingly simple technique for improving the robustness of neural networks against black-box attacks. The proposed method creates a *fixed* random mask to zero out lower layer activations during training and test. Extensive experiments show that the proposed method without adversarial training is competitive with a state-of-the-art defense method under blackbox attacks.\n\nPros:\n -- simplicity and effectiveness of the method\n -- extensive experimental results under different settings\n\nCons:\n -- it's not clear why the method works besides some not-yet-validated hypotheses.\n -- graybox results seem to suggest that the effectiveness of the method is due to the baseline CNNs and the proposed CNNs learning very different functions; source models within the same family still produce strong transferable attacks. It would have been much more impressive if different randomness could result in very different functions, leading to strong defense in the graybox setting.",
        "title": "interesting observations; but what insights to get out of it?"
    },
    "review_2": {
        "confidence": "3: The reviewer is fairly confident that the evaluation is correct",
        "rating": "7: Good paper, accept",
        "review": "The authors propose a simple method for increasing the robustness of convolutional neural networks against adversarial examples. This method is simple but seems to achieve surprisingly good results. It consist in randomly remove neurons from the network architecture. The deleted neurons are selected before training and remain deleted during the training and test phase.  The authors also study the adversarial examples that still fool the network after applying their method and find than those examples also fool human. This finding raises the question of what is an adversarial example if both humans and networks are fooled by the same example. \n\nUsing Random Masks in neural network is not a new idea since it was already proposed for DropOut or DropConnect (Regularization of Neural Networks using DropConnect, ICML2013) and in the context of adversarial attacks (Dhillon et al. 2018)  as reported by the authors. The discussion (Section 2) about the impact of random masks on what convolution layers capture in the spatial organisation of the input is interesting: whereas standard CNNs focus on detecting the presence of a feature in the output, random mask could force the CNN layers to learn how a specific feature distributes on the whole input maps. This limitation of the CNN has already been pointed up and solutions have been proposed for example Capsule Networks (Dynamic Routing Between Capsules, NIPS 2017). This intuition is experimentally supported by a simple random shuffle by block of the input image  (Appendix A).\n\nIn Section 3, the authors present a large number of experiments to demonstrate the robustness of their method. Most of the details are given in the 13 (!) pages of appendix. Experiments against black-box attack, random noise, white-box attack, grey-box are presented. Most of the experiments are on CIFAR10 but one experiment is also presented on MNIST. One could regret that only one architecture of CNN is tested (ResNet18) except for gray-box attack, for which DenseNet121 and VG19 are tested. One could ask why the type of models tested is not consistent across the different experiments.  For black-box attack, random masks compare favourably to Madry\u2019s defence. For white box defence, Random Mask is not compared to another defence method, which seems a weakness to me but I am not familiar enough with papers in this area to estimate if this is a common practice. In most of the experiments, the drop ratio is between 0.5 and 0.9, which seems to indicate that the size the initial network could be reduced by more than 50% to increase the robustness to attack. This ratio is larger than what is usually used for dropout (0.5 at most).  \n\nIn section 3.3, different strategies for random masks are explored : where to apply random masks, random mask versus random channels, random masks versus same masks. Results are given in table 2. The caption of Table 2 could be more explicit : what are the presented percent ?\n\nExperiments on masking shallow versus deep layers are interesting. Best results for robustness are obtained with masking shallow layers at quite a high ratio (0.9). One could ask if this result could be due to the type or the parameters of adversarial attacks which are not adapted to such a high sparseness on shallow layers or to the specific kind of sparseness induced by the masks. A comparison to a regular network with the same number of free parameters as the masked network could give insight on this aspect. \n\npros : simple to implement, good robustness shown agains a variety of attack types\ncons : mainly tested on a single architecture (ResNet) and on a single datatbase CIFAR. Maybe not robust against the latest techniques of adversarial attack.",
        "title": "Simple but efficient method to increasing the robustness of CNN against adversarial attacks"
    }
}